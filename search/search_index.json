{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Hands On GCP - Closer Academy","text":""},{"location":"#introduction","title":"Introduction","text":"<p>This repository contains the Hands On GCP code for the workshops within Closer Academy .</p> <p>This repository has the following workshops:</p> <ul> <li>Building a simple MLOps system in Google Cloud Platform.<ul> <li>By end end of it, we should be able to understand how to create resources in Google Cloud, how to use them and how to integrate them with each other.</li> </ul> </li> </ul>"},{"location":"#setup","title":"Setup","text":"<p>Use the Cloud Editor to copy this repository.</p> <p>Click the Cloud Shell Icon:</p> <p></p> <p>Something like this should pop-up:</p> <p></p> <p>After that, clone the git repository and change directory to the cloned repository. In the exercises, you'll change between the Cloud Editor and the Cloud Shell.</p> <p></p>"},{"location":"#development-requirements-and-setup","title":"Development - Requirements and Setup","text":"<p>An IDE, we recommend VSCode, but you can use any you like. (Althought there's a devcontainer you can use to power-up the repository with the necessary requirements faster.)</p> <ul> <li>Visual Studio Code<ul> <li>Download for windows: https://code.visualstudio.com/download</li> </ul> </li> </ul>"},{"location":"#in-windows","title":"In Windows","text":"<ul> <li>WSL (Windows Linux Subsystem)<ul> <li>Installation steps: https://learn.microsoft.com/en-us/windows/wsl/install</li> </ul> </li> </ul>"},{"location":"#for-the-devcontainer","title":"For The devcontainer","text":"<ul> <li>Docker<ul> <li>Download for windows: https://www.docker.com/products/docker-desktop/</li> </ul> </li> </ul> <p>This repository has a devcontainer, which you can use to keep You can learn more about Devcontainers here.</p>"},{"location":"#exercises","title":"Exercises","text":"<ul> <li>Simple MLOps Pipelines</li> </ul>"},{"location":"client_libs_py/","title":"Client Libraries","text":"<p>A client implementation, such as a Python Client or Javascript Client, is a software library designed to facilitate communication and interaction between an application and a specific service, like an API. It allows developers to easily access and utilize the service's functionalities, by abstracting low-level details and providing a more user-friendly interface in the language of choice.</p> <p>The client implementation acts as an API layer between the application and the server, enabling seamless data exchange and requests management. This layer simplifies the process of making API calls, handling authentication, managing connection details, and processing responses from the server.</p> <p>For example, the Google Cloud Platform (GCP) offers BigQuery Python Client and Python Cloud Storage as part of their Cloud Client Libraries. These libraries provide high-level API abstractions that significantly reduce the amount of boilerplate code developers need to write when interacting with BigQuery and Cloud Storage services.</p> <p>Using the GCP BigQuery Python Client, developers can easily query, manage, and load data into BigQuery tables, while the Python Cloud Storage library simplifies file management, uploads, and downloads in Google Cloud Storage. Both libraries embrace the idiomatic style of the Python language, ensuring better integration with the standard library and developers' existing codebases.</p> <p>You can check all the available Client Libraries for python here.</p>"},{"location":"client_libs_py/#bigquery-client-python","title":"Bigquery Client (Python)","text":"<p>You can use the BigQuery Python Client to execute a query and fetch the results:</p> <pre><code># NOTE: pip install google-cloud-bigquery\n\nfrom google.cloud import bigquery\n\n# Initialize the BigQuery client\nclient = bigquery.Client()\n\n# Define your query\nquery = \"\"\"\n    SELECT name, SUM(number) as total\n    FROM `bigquery-public-data.usa_names.usa_1910_current`\n    WHERE year &gt;= 2000\n    GROUP BY name\n    ORDER BY total DESC\n    LIMIT 10\n\"\"\"\n\n# Execute the query\nquery_job = client.query(query)\n\n# Fetch and print the results\nfor row in query_job.result():\n    print(f\"{row.name}: {row.total}\")\n</code></pre>"},{"location":"client_libs_py/#cloud-storage-client-python","title":"Cloud Storage Client (Python)","text":"<p>You can use the Python Cloud Storage Client to upload a file to a GCS bucket and download it back:</p> <pre><code># NOTE: pip install google-cloud-storage\n\nfrom google.cloud import storage\n\n# Initialize the GCS client\nclient = storage.Client()\n\n# Specify your bucket name\nbucket_name = \"your-bucket-name\"\n\n# Get a reference to the bucket\nbucket = client.get_bucket(bucket_name)\n\n# Upload a file\nsource_file_name = \"path/to/your/local/file.txt\"\ndestination_blob_name = \"uploaded_file.txt\"\nblob = bucket.blob(destination_blob_name)\nblob.upload_from_filename(source_file_name)\nprint(f\"File {source_file_name} uploaded to {destination_blob_name}.\")\n\n# Download the file\ndownloaded_file_name = \"path/to/your/local/downloaded_file.txt\"\nblob = bucket.blob(destination_blob_name)\nblob.download_to_filename(downloaded_file_name)\nprint(f\"File {destination_blob_name} downloaded to {downloaded_file_name}.\")\n</code></pre>"},{"location":"client_libs_py/#pubsub-client-python","title":"Pub/Sub Client (Python)","text":"<p>You can use the Pub/Sub Python Client to publish a message to the existing topic:</p> <pre><code># NOTE: pip install google-cloud-pubsub\n\nfrom google.cloud import pubsub_v1\n\n# Initialize the Pub/Sub client\npublisher = pubsub_v1.PublisherClient()\n\n# Set your project_id and topic_name\nproject_id = \"your-project-id\"\ntopic_name = \"your-existing-topic-name\"\n\n# Get the topic path\ntopic_path = publisher.topic_path(project_id, topic_name)\n\n# Publish a message\nmessage = \"Hello, World!\"\nmessage_data = message.encode(\"utf-8\")\nfuture = publisher.publish(topic_path, message_data)\nmessage_id = future.result()\nprint(f\"Message published with ID: {message_id}\")\n</code></pre> <p>As Pub/Sub promotes decoupled and flexible architectures, message_data is transformed into a base64-encoded string to ensure language-agnostic compatibility. Therefore, subscribers must decode the base64 message. In Python, this can be done as follows:</p> <pre><code>import base64\n\ndef hello_pubsub(data, context):\n    \"\"\"Triggered from a message on a Cloud Pub/Sub topic.\n    Args:\n         event (dict): Event payload.\n         context (google.cloud.functions.Context): Metadata for the event.\n    \"\"\"\n    print(\"This Function was triggered by messageId {} published at {}\".format(context.event_id, context.timestamp))\n\n    if 'data' in event:\n        decoded_msg = base64.b64decode(data['data']).decode('utf-8')\n        # Message is now decoded\n    ## Your Cloud Function Implementation\n</code></pre>"},{"location":"client_libs_py/#setup-python-venv","title":"Setup python venv","text":"<pre><code>python -m venv venv\n</code></pre> <p>Install the Python Extension:</p> <p></p> <p>With VSCode, do <code>CTRL+SHIFT+P</code> and write <code>Select Interpreter</code></p> <p></p> <p>And find the <code>venv</code> python executable.</p> <p></p>"},{"location":"exercises/simple_mlops/","title":"MLOps Pipelines with Cloud Functions","text":""},{"location":"exercises/simple_mlops/#requirements","title":"Requirements","text":"<ul> <li>A Google Cloud Project Environment.</li> </ul>"},{"location":"exercises/simple_mlops/#architecture","title":"Architecture","text":"<p>Please, refer to this document for a detailed explanation of the architecture of the first project.</p>"},{"location":"exercises/simple_mlops/#google-cloud-services","title":"Google Cloud Services","text":"<p>Please, refer to this document for an introduction to some of the Google Cloud services used in this workshop.</p>"},{"location":"exercises/simple_mlops/architecture/","title":"Architecure","text":"<p>We are going to build a simple MLOps project in Google Cloud Platform using <code>Cloud Storage</code>, <code>Cloud Functions</code>, <code>Bigquery</code> and <code>Pubsub</code>.</p> <p>Our minimal MLOps system should look like this in the end:</p> <p></p>"},{"location":"exercises/simple_mlops/architecture/#1-ingestion","title":"1. Ingestion","text":"<ol> <li>Cloud Function <code>Ingest Data</code> monitors the <code>yourname-lz</code> for new files.</li> <li>Upon detecting a new file, <code>Ingest Data</code> writes its contents to the BigQuery table <code>Titanic Raw</code>.</li> <li>Once sucessufully complete, a message is sent to the <code>yourname-ingestion-complete</code> topic, notifying subscribers about the new data in BigQuery.</li> </ol>"},{"location":"exercises/simple_mlops/architecture/#2-staging-to-facts","title":"2. Staging to Facts","text":"<ol> <li>The function <code>Query to Facts</code> is activated, and executes a query which moves new data from the <code>Titanic Raw</code> table to <code>Titanic Facts</code>.</li> <li>Once sucessfully complete, a message is sent to the topic <code>yourname-update-facts-complete</code>, notifying its subscribers that the move from raw to facts is complete.</li> </ol>"},{"location":"exercises/simple_mlops/architecture/#3-train-model","title":"3. Train model","text":"<ol> <li>The <code>train_model</code> Cloud Function, is activated by the topic<code>yourname-update-facts-complete</code>, and pulls the data from the bigquery table <code>Titanic Facts</code>.</li> <li>Once the training of the model is done, the model file is uplodaded to the <code>yourname-models</code> bucket.</li> </ol>"},{"location":"exercises/simple_mlops/architecture/#4-predictions","title":"4. Predictions","text":"<ol> <li>A model is retrieved from the  <code>yourname-models</code> bucket and lodaded into the Cloud Function. The function is ready to take requests.</li> <li>A request for a prediction is made by an user.</li> <li>The request is saved to the <code>Titanic Predictions</code> table</li> <li>A response is given back to the user.</li> </ol>"},{"location":"exercises/simple_mlops/dataset/","title":"Meet the dataset","text":"<p>We will use the Titanic Dataset available pretty much anywhere.</p> <p>The columns and their types are the following</p> Column name Python data type Bigquery data type Description PassengerId int INT64 Unique identifier for each passenger Survived bool BOOLEAN Survival status (False = No, True = Yes) Pclass int INT64 Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd) Name str STRING Full name of the passenger Sex str STRING Gender (male or female) Age float FLOAT64 Age in years SibSp int INT64 Number of siblings/spouses aboard the Titanic Parch int INT64 Number of parents/children aboard the Titanic Ticket str STRING Ticket number Fare float FLOAT64 Passenger fare Cabin str STRING Cabin number Embarked str STRING Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) OPTIONAL: set_type str STRING Set type (Train / Test / Validation) <p>So, when creating the Tables, you have to create the schema accodingly. </p> <p>The dataset is available at <code>./dataset/titanic.csv</code>.</p>"},{"location":"exercises/simple_mlops/step1/","title":"Load a file from Cloud Storage to a Bigquery Table using a Cloud Function","text":"<ul> <li>Load a file from Cloud Storage to a Bigquery Table using a Cloud Function</li> <li>Introduction</li> <li>Tasks</li> <li>Create the Google Cloud Resources<ul> <li>1. Create a BigQuery Dataset</li> <li>2. Create a BigQuery Table</li> <li>3. Create a Google Cloud Storage Bucket</li> <li>4. Create the pubsub topic for ingestion complete</li> </ul> </li> <li>Cloud Function<ul> <li>Update the Cloud Function Code</li> <li>Deploy the cloud function</li> </ul> </li> <li>Ingest the data</li> <li>Documentation</li> </ul>"},{"location":"exercises/simple_mlops/step1/#introduction","title":"Introduction","text":"<p>In this exercise, we will create the <code>Ingest Data</code> Cloud Function, that will perform the following tasks:</p> <ol> <li> <p>The <code>Ingest Data</code> function will actively monitor the <code>[YOURNAME]-lz</code> Google Cloud Storage bucket for new files. This is achieved by configuring a trigger topic (PubSub) in the Cloud Function to listen for object creation events in the specified bucket.</p> </li> <li> <p>When a new file is detected, the <code>Ingest Data</code> function will read the contents of the file and write the data into a BigQuery table named <code>Titanic Raw</code>. The function will leverage the BigQuery Python client library to facilitate this process, efficiently importing the data from the file into the specified table.</p> </li> <li> <p>After successfully importing the data into BigQuery, the <code>Ingest Data</code> function will send a message to the <code>yourname-ingestion-complete</code> topic in Google Cloud Pub/Sub. This message will notify all subscribers that new data has been loaded into BigQuery, allowing them to react accordingly, such as by initiating further data processing tasks.</p> </li> </ol> <p>The Cloud Function <code>Ingest Data</code> will utilize the Google Cloud Storage, BigQuery, and Pub/Sub client libraries for these tasks.</p> <p>The resources needed these tasks are:</p> <ul> <li>One Bigquery Data Set and one bigquery Table</li> <li>The table schema is at: <code>./resources/mlops_usecase/bigquery/titanic_schema_raw.json</code></li> <li>One GCS Bucket named <code>[your_name]-lz</code> where you will drop the files once the function is ready</li> <li>One Topic named <code>[your_name]-ingestion-complete</code>, to where the function will send a message once complete.</li> </ul> <p>The outline of the Cloud Function code is available at <code>functions/simple_mlops/a_ingest_data/app/main.py</code>.</p> <pre><code>.\n\u2514\u2500\u2500 a_ingest_data/\n    \u251c\u2500\u2500 app/\n    \u2502   \u251c\u2500\u2500 funcs/\n    \u2502   \u2502   \u251c\u2500\u2500 models.py # Models to make typechecking easier.\n    \u2502   \u2502   \u251c\u2500\u2500 gcp_apis.py # Functions to call google services.\n    \u2502   \u2502   \u2514\u2500\u2500 transform.py # Transformations of data into structures\n    \u2502   \u251c\u2500\u2500 main.py # Main module and entry point for the Cloud Function\n    \u2502   \u2514\u2500\u2500 requirements.txt # Requirements for the function execution.\n    \u251c\u2500\u2500 config/\n    \u2502   \u2514\u2500\u2500 dev.env.yaml # Environment variables that will ship with the function deployment\n    \u2514\u2500\u2500 tests/\n        \u2514\u2500\u2500 test_*.py # Unit tests.\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#tasks","title":"Tasks","text":"<ul> <li> Create the Google Cloud Resources</li> <li> Update the Cloud Function Code</li> <li> Deploy the Cloud Function</li> <li> Test the Cloud Function</li> </ul>"},{"location":"exercises/simple_mlops/step1/#create-the-google-cloud-resources","title":"Create the Google Cloud Resources","text":"<p>Here are the resources necessary to complete the exercise:</p> <p>You can create the resources with Cloud Shell or in the Console. ***The end result will be the same. When creating a resource, choose either to create it with the cloud shell or the console, but not both.</p> <p>We recommend using the UI to get a better understanding of GCP.***</p> <p>For Cloud Shell, set these variables:</p> <pre><code>export PROJECT_ID=$(gcloud config get-value project)\nexport PROJECT_NAME=$(gcloud config get-value project)\nexport PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format='value(projectNumber)')\nexport REGION=europe-west3\nexport YOURNAME=your_name_in_lowercase\n</code></pre> <p></p>"},{"location":"exercises/simple_mlops/step1/#1-create-a-bigquery-dataset","title":"1. Create a BigQuery Dataset","text":"<p>Create with either Cloud Shell OR the Console UI.</p> <p>With the Console:</p> <ol> <li> <p>Go to BigQuery:</p> <p></p> </li> <li> <p>Click the bullet points icon next to the project name:</p> <p></p> </li> <li> <p>Name your data set, change the region, and click <code>CREATE DATA SET</code>:</p> <p></p> <p>Congratulations! You have a <code>data set</code>!</p> </li> <li> <p>Edit the labels</p> <p>Click in the recently created dataset. </p> <p>And add the labels</p> <p></p> </li> </ol> <p>With Cloud Shell, execute the following command:</p> <pre><code>bq mk \\\n    --project_id ${PROJECT_ID} \\\n    --location ${REGION} \\\n    --dataset \\\n    --description \"Dataset for the Titanic dataset\" \\\n    --label=owner:${YOURNAME} \\\n    --label=project:${PROJECT_NAME} \\\n    --label=purpose:academy \\\n    ${YOURNAME}_titanic\n</code></pre> <p>Reference: bq mk --dataset</p>"},{"location":"exercises/simple_mlops/step1/#2-create-a-bigquery-table","title":"2. Create a BigQuery Table","text":"<p>Create with either Cloud Shell OR the Console UI.</p> <p>With the console:</p> <ol> <li> <p>Click the bullets icon next to your data set, and click Create Table:</p> <p></p> </li> <li> <p>Configure your table:</p> <p></p> <ol> <li>Make sure it's in your dataset created in the step before</li> <li>Name your dataset <code>titanic_raw</code></li> <li>Copy the schema in the repository folder <code>resources/mlops_example/bigquery/titanic_schema_raw.json</code> and paste it</li> <li>Create the table.</li> </ol> </li> <li> <p>Add the labels.</p> <p></p> <p>To add the labels go to <code>EDIT DETAILS</code>, and the same way as the dataset, add the labels. Include the <code>Dataset</code> : <code>titanic</code> label.</p> </li> </ol> <p>With Cloud Shell, execute the following command:</p> <pre><code>bq mk \\\n    --project_id ${PROJECT_ID} \\\n    --table \\\n    --description \"Table for the Titanic dataset\" \\\n    --label=owner:${YOURNAME} \\\n    --label=project:${PROJECT_NAME} \\\n    --label=purpose:academy \\\n    --label=dataset:titanic \\\n    ${YOURNAME}_titanic.titanic_raw \\\n    ./resources/mlops_usecase/bigquery/titanic_schema_raw.json\n</code></pre> <p>Reference: bq mk --table</p>"},{"location":"exercises/simple_mlops/step1/#3-create-a-google-cloud-storage-bucket","title":"3. Create a Google Cloud Storage Bucket","text":"<p>Create with either Cloud Shell OR the Console UI.</p> <p>With the console:</p> <ol> <li> <p>Search for the Cloud Storage in the Search bar.</p> <p></p> </li> <li> <p>In the Cloud Storage UI, you'll notice there are no buckets created yet. To create one, click the <code>CREATE</code> button.</p> <p></p> </li> <li> <p>Configurate your bucket</p> <p></p> <ol> <li>Name your bucket and click Continue.</li> <li>Change the storage class from Multi-region to Region. Set the location to europe-west3, as shown in the image, and click Continue.</li> <li>Keep the remaining settings as they are.</li> <li>Click create.</li> </ol> <p>Your configuration should look like this:</p> <p></p> <p>If this popup appears, leave the settings as they are.</p> <p></p> </li> </ol> <p>With Cloud Shell, execute the following command:</p> <pre><code>gsutil mb \\\n    -c regional \\\n    -l ${REGION} \\\n    -p ${PROJECT_ID} \\\n    gs://${YOURNAME}-lz\n\ngsutil label ch -l owner:${YOURNAME} gs://${YOURNAME}-lz\ngsutil label ch -l project:${PROJECT_NAME} gs://${YOURNAME}-lz\ngsutil label ch -l purpose:academy gs://${YOURNAME}-lz\n</code></pre> <p>Reference: gsutil mb, gsutil label</p> <p>And now you have your bucket!</p> <p></p> <p>Alternatively, you can create a bucket using Python, other Client Libraries, or even advanced Infrastructure-as-Code tools like Terraform or Pulumi.</p>"},{"location":"exercises/simple_mlops/step1/#4-create-the-pubsub-topic-for-ingestion-complete","title":"4. Create the pubsub topic for ingestion complete","text":"<p>Create with either Cloud Shell OR the Console UI.</p> <p>With the Cloud Console:</p> <ol> <li>Search for Topics in the search bar.</li> <li> <p>Click in CREATE TOPIC.</p> <p></p> </li> <li> <p>Define your Topic ID and click CREATE</p> <p>The topic ID should be <code>[your_name]-ingestion_complete</code></p> <p></p> <p>In this case, our Topic ID is <code>ingestion_complete</code>.</p> <p>Remember where to find your Topic IDs, it will be useful when instrumenting the python scripts.</p> </li> <li> <p>Verify your topic was created</p> </li> </ol> <p></p> <p>It automatically creates a subscription, but lets ignore that for now.</p> <p>With Cloud Shell:</p> <pre><code>gcloud pubsub topics create ${YOURNAME}-ingestion-complete \\\n    --project=${PROJECT_ID} \\\n    --labels=owner=${YOURNAME},project=${PROJECT_NAME},purpose=academy\n</code></pre> <p>Now we are ready to move to the cloud function code.</p>"},{"location":"exercises/simple_mlops/step1/#cloud-function","title":"Cloud Function","text":""},{"location":"exercises/simple_mlops/step1/#update-the-cloud-function-code","title":"Update the Cloud Function Code","text":"<p>Here are the steps necessary to complete the exercise:</p> <ol> <li> <p>Set Environment Variables</p> <p>In the <code>functions/mlops_usecase/a_ingest_data/config/dev.env.yaml</code> file, change the environment variables for the correct ones.</p> <pre><code>_GCP_PROJECT_ID: \"The GCP project ID where the resources are located\"\n_BIGQUERY_DATASET_ID: \"The BigQuery dataset ID you created\"\n_BIGQUERY_TABLE_ID: \"The BigQuery table ID where you will store the data\"\n_TOPIC_INGESTION_COMPLETE: \"The Pub/Sub topic ID where you will send a message once the data is ingested\"\n</code></pre> </li> <li> <p>Publish the message: To verify you concluded this step with success, change the string in the <code>'closer-origin-function'</code> to <code>'functions.mlops.ingest_data'</code></p> <pre><code>########################################\n# 2. Send the verification attribute ###\n########################################\n\n_ = gcp_apis.pubsub_publish_message(\n    PS=gcp_clients.publisher,\n    project_id=env_vars.gcp_project_id,\n    topic_id='verification',\n    message='ok',\n    attributes={\n    'closer-origin-function': 'functions.mlops.ingest_data',\n    'closer-origin-topic': env_vars.topic_ingestion_complete,\n    },\n)\n</code></pre> </li> </ol>"},{"location":"exercises/simple_mlops/step1/#deploy-the-cloud-function","title":"Deploy the cloud function","text":"<p>You can check the deployment here in Cloud Build</p> <pre><code># Remeber to have $YOURNAME from the first export to the Cloud Shell. \n# Uncomment the next lines if you see necessary\n# export REGION=europe-west3\n# export YOURNAME=your_name_in_lowercase\nexport FUNCTION_NAME=\"ingest_data\"\nexport PATH_TO_FUNCTION=\"functions/mlops_usecase/a_ingest_data\"\n\ngcloud beta functions deploy ${YOURNAME}-${FUNCTION_NAME} \\\n    --gen2 --cpu=1 --memory=512MB \\\n    --region=${REGION} \\\n    --runtime=python311 \\\n    --source=${PATH_TO_FUNCTION}/app/ \\\n    --env-vars-file=${PATH_TO_FUNCTION}/config/dev.env.yaml \\\n    --entry-point=main \\\n    --trigger-event-filters=\"type=google.cloud.storage.object.v1.finalized\" \\\n    --trigger-event-filters=\"bucket=${YOURNAME}-lz\"\n</code></pre> <p>Reference: gcloud functions deploy</p> <p>This code deploys a Google Cloud Function named \"ingest_data\" using the gcloud command-line tool. The function is written in Python 3.11 and is triggered by a Google Cloud Storage object finalization event in the \"${YOURNAME}-lz\" bucket.</p> <p>The function is deployed with the following configuration:</p> <ul> <li>CPU: 1</li> <li>Memory: 512MB</li> <li>Region: $REGION</li> <li>Runtime: python311</li> <li>Source code location: \"${PATH_TO_USECASE}/app/\"</li> <li>Environment variables: loaded from \"${PATH_TO_USECASE}/config/dev.env.yaml\"</li> <li>Entry point: \"main\"</li> <li>Trigger: Google Cloud Storage object finalization event in the \"${YOURNAME}-lz\" bucket</li> </ul>"},{"location":"exercises/simple_mlops/step1/#ingest-the-data","title":"Ingest the data","text":"<p>Go to the Cloud Functions UI in the console, you can confirm the cloud function was deployed successfully. (By searching in the search box), you can check if the deployment was correctly made.</p> <p></p> <p>To add the data to the created cloud storage bucket, you have two options.</p> <ol> <li>Transfer the csv to your local machine and upload it to the bucket.</li> <li>The data is in the folder <code>./resources/mlops_usecase/data/titanic.csv</code></li> <li>Go to the Cloud Storage UI.</li> <li>Click on the bucket you created.</li> <li> <p>Click on Upload Files and select the file. </p> </li> <li> <p>Use the Cloud Shell to copy the file from the running environment to the cloud storage bucket like so:</p> <pre><code>gsutil cp resources/mlops_usecase/data/titanic.csv gs://${YOURNAME}-lz/\n</code></pre> </li> </ol> <p>After this, you can verify the data was ingested correctly.</p> <ol> <li> <p>Go to the BigQuery UI, click your dataset and table, and verify the data is there (Either go to the <code>Preview</code> tab or run a query against the table).    </p> </li> <li> <p>You can also check the function logs to see if there was no error.</p> </li> <li>Go to the Cloud Functions UI.</li> <li>Click on the function name.</li> <li>Click in the logs tab<ol> <li>This is bad: </li> <li>This is good: </li> </ol> </li> </ol> <p>And this first phase is complete. Congratulations!</p> <p>If you have any questions, please reach out to the tutors.</p>"},{"location":"exercises/simple_mlops/step1/#documentation","title":"Documentation","text":""},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.main","title":"<code>main</code>","text":"<p>Cloud Function to Ingest Data.</p>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.main.load_clients","title":"<code>load_clients(gcp_project_id)</code>","text":"<p>Load the GCP clients.</p> <p>Parameters:</p> Name Type Description Default <code>gcp_project_id</code> <code>str</code> <p>The GCP project ID.</p> required <p>Returns:</p> Name Type Description <code>GCPClients</code> <code>GCPClients</code> <p>A tuple of GCP clients. With the following attributes:     storage_client: A storage client.     bigquery_client: A bigquery client.     publisher: A pubsub publisher client.</p> Source code in <code>functions/mlops_usecase/a_ingest_data/app/main.py</code> <pre><code>def load_clients(gcp_project_id: str) -&gt; models.GCPClients:\n\t\"\"\"Load the GCP clients.\n\n\tArgs:\n\t    gcp_project_id (str): The GCP project ID.\n\n\tReturns:\n\t    GCPClients: A tuple of GCP clients.\n\t        With the following attributes:\n\t            storage_client: A storage client.\n\t            bigquery_client: A bigquery client.\n\t            publisher: A pubsub publisher client.\n\t\"\"\"\n\tstorage_client = storage.Client(project=gcp_project_id)\n\tbigquery_client = bigquery.Client(project=gcp_project_id)\n\tpublisher = pubsub.PublisherClient()\n\n\treturn models.GCPClients(storage_client=storage_client, bigquery_client=bigquery_client, publisher=publisher)\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.main.main","title":"<code>main(cloud_event)</code>","text":"<p>Entrypoint of the cloud function.</p> <p>Parameters:</p> Name Type Description Default <code>cloud_event</code> <code>CloudEvent</code> <p>The cloud event that triggered this function.</p> required Source code in <code>functions/mlops_usecase/a_ingest_data/app/main.py</code> <pre><code>@functions_framework.cloud_event\ndef main(cloud_event: CloudEvent) -&gt; None:\n\t\"\"\"Entrypoint of the cloud function.\n\n\tArgs:\n\t    cloud_event (CloudEvent): The cloud event that triggered this function.\n\t\"\"\"\n\trun_hash = str(uuid.uuid4())\n\tif not hasattr(main, 'env_vars'):\n\t\tenv_vars = _env_vars()\n\n\tif not hasattr(main, 'gcp_clients'):\n\t\tgcp_clients = load_clients(gcp_project_id=env_vars.gcp_project_id)  # type: ignore\n\n\t# Get the data from the cloud event\n\tdata: dict = cloud_event.get_data()  # type: ignore\n\n\tfile_contents = gcp_apis.storage_download_blob_as_string(\n\t\tCS=gcp_clients.storage_client,  # type: ignore\n\t\tbucket_name=data['bucket'],\n\t\tfile_path=data['name'],\n\t)\n\n\t# Split the content by lines\n\tdatapoints = transform.split_lines(content=file_contents)\n\n\t# Get the headers (column names) from the first line\n\thas_headers = True\n\tif has_headers:\n\t\tdatapoints = datapoints[1:]\n\n\t# Iterate through the the datapoints and insert them into BigQuery\n\terrors = [\n\t\tgcp_apis.bigquery_insert_json_row(\n\t\t\tBQ=gcp_clients.bigquery_client,\n\t\t\ttable_fqn=env_vars.bq_table_fqn,\n\t\t\trow=[datapoint.to_dict()],\n\t\t)\n\t\tfor datapoint in transform.titanic_transform(\n\t\t\trun_hash=run_hash,\n\t\t\tdatapoints=datapoints,\n\t\t)\n\t]\n\n\tif any(errors):\n\t\traise ValueError(f'Errors found: {errors}')\n\n\t_ = gcp_apis.pubsub_publish_message(\n\t\tPS=gcp_clients.publisher,\n\t\tproject_id=env_vars.gcp_project_id,\n\t\ttopic_id=env_vars.topic_ingestion_complete,\n\t\tmessage=f\"I finished ingesting the file {data['name']}!!\",\n\t\tattributes={\n\t\t\t'closer-origin-function': 'functions.mlops.ingest_data',\n\t\t\t'closer-run-hash': run_hash,\n\t\t},\n\t)\n\n\t########################################\n\t# 2. Send the verification attribute ###\n\t########################################\n\n\t_ = gcp_apis.pubsub_publish_message(\n\t\tPS=gcp_clients.publisher,\n\t\tproject_id=env_vars.gcp_project_id,\n\t\ttopic_id='verification',\n\t\tmessage='ok',\n\t\tattributes={\n\t\t\t'closer-origin-function': 'functions.mlops.ingest_data',\n\t\t\t'closer-origin-topic': env_vars.topic_ingestion_complete,\n\t\t},\n\t)\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.gcp_apis","title":"<code>gcp_apis</code>","text":"<p>This module contains the functions that interact with the GCP APIs.</p>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.gcp_apis.bigquery_insert_json_row","title":"<code>bigquery_insert_json_row(BQ, table_fqn, row)</code>","text":"<p>Inserts a row into a bigquery table.</p> <p>Parameters:</p> Name Type Description Default <code>BQ</code> <code>Client</code> <p>The bigquery client.</p> required <code>table_fqn</code> <code>str</code> <p>The fully qualified name of the table.</p> required <code>row</code> <code>Dict[str, Any]</code> <p>The row to insert into the table.</p> required Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/gcp_apis.py</code> <pre><code>def bigquery_insert_json_row(\n\tBQ: bigquery.Client,\n\ttable_fqn: str,\n\trow: Sequence[Dict[str, Any]],\n) -&gt; Any:\n\t\"\"\"Inserts a row into a bigquery table.\n\n\tArgs:\n\t    BQ (bigquery.Client): The bigquery client.\n\t    table_fqn (str): The fully qualified name of the table.\n\t    row (Dict[str, Any]): The row to insert into the table.\n\t\"\"\"\n\n\tdef _filter_dict(d: Dict[str, str]) -&gt; Dict[str, str]:\n\t\treturn {k: v for k, v in d.items() if isinstance(v, str) and bool(v.strip())}\n\n\tif not isinstance(row, Sequence) and isinstance(row, Dict):\n\t\trow = [row]\n\n\terrors = BQ.insert_rows_json(\n\t\ttable=table_fqn,\n\t\tjson_rows=[_filter_dict(d=d) for d in row],\n\t)\n\n\tif errors:\n\t\tprint(json.dumps({'message': errors, 'severity': 'ERROR'}))\n\t\treturn errors\n\telse:\n\t\treturn None\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.gcp_apis.pubsub_publish_message","title":"<code>pubsub_publish_message(PS, project_id, topic_id, message, attributes={})</code>","text":"<p>Publishes a message to a Pub/Sub topic.</p> <p>Parameters:</p> Name Type Description Default <code>PS</code> <code>PublisherClient</code> <p>The pubsub client.</p> required <code>project_id</code> <code>str</code> <p>The ID of the project where the topic is located.</p> required <code>topic_id</code> <code>str</code> <p>The ID of the topic.</p> required <code>message</code> <code>str</code> <p>The message to publish.</p> required <code>attributes</code> <code>Dict[str, str]</code> <p>The attributes of the message. Defaults to {}.</p> <code>{}</code> Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/gcp_apis.py</code> <pre><code>def pubsub_publish_message(\n\tPS: pubsub.PublisherClient,\n\tproject_id: str,\n\ttopic_id: str,\n\tmessage: str,\n\tattributes: Dict[str, str] = {},\n) -&gt; None:\n\t\"\"\"Publishes a message to a Pub/Sub topic.\n\n\tArgs:\n\t    PS (pubsub.PublisherClient): The pubsub client.\n\t    project_id (str): The ID of the project where the topic is located.\n\t    topic_id (str): The ID of the topic.\n\t    message (str): The message to publish.\n\t    attributes (Dict[str, str], optional): The attributes of the message.\n\t        Defaults to {}.\n\t\"\"\"\n\t_topic = PS.topic_path(project_id, topic_id)\n\n\tPS.publish(\n\t\ttopic=_topic,\n\t\tdata=message.encode('utf-8'),\n\t\t**attributes,\n\t)\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.gcp_apis.storage_download_blob_as_string","title":"<code>storage_download_blob_as_string(CS, bucket_name, file_path)</code>","text":"<p>Downloads a blob from a Google Cloud Storage bucket and returns its content as a string.</p> <p>Parameters:</p> Name Type Description Default <code>CS</code> <code>Client</code> <p>A Google Cloud Storage client object.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket.</p> required <code>file_path</code> <code>str</code> <p>The location of the blob/file inside the bucket.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string with the file content.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the blob does not exist.</p> Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/gcp_apis.py</code> <pre><code>def storage_download_blob_as_string(\n\tCS: storage.Client,\n\tbucket_name: str,\n\tfile_path: str,\n) -&gt; str:\n\t\"\"\"Downloads a blob from a Google Cloud Storage bucket and returns its content as a string.\n\n\tArgs:\n\t    CS (google.cloud.storage.Client): A Google Cloud Storage client object.\n\t    bucket_name (str): The name of the bucket.\n\t    file_path (str): The location of the blob/file inside the bucket.\n\n\tReturns:\n\t    str: A string with the file content.\n\n\tRaises:\n\t    ValueError: If the blob does not exist.\n\t\"\"\"\n\t# Getting the bucket\n\tbucket = CS.bucket(bucket_name)\n\n\t# Getting the blob\n\tblob = bucket.blob(file_path)\n\n\tif blob.exists():\n\t\t# Downloading the blob\n\t\treturn blob.download_as_text()\n\telse:\n\t\traise ValueError(f'Blob {file_path} does not exist.')\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.transform","title":"<code>transform</code>","text":""},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.transform.split_lines","title":"<code>split_lines(content)</code>","text":"<p>Split the content by lines.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content of the file.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: The lines of the file.</p> Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/transform.py</code> <pre><code>def split_lines(\n\tcontent: str,\n) -&gt; List[str]:\n\t\"\"\"Split the content by lines.\n\n\tArgs:\n\t    content (str): The content of the file.\n\n\tReturns:\n\t    List[str]: The lines of the file.\n\t\"\"\"\n\treturn content.strip().split('\\n')\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.transform.titanic_transform","title":"<code>titanic_transform(run_hash, datapoints)</code>","text":"<p>Generator that transforms a CSV datapoint into a titanic data object.</p> <p>Parameters:</p> Name Type Description Default <code>run_hash</code> <code>str</code> <p>The hash of the run.</p> required <code>datapoints</code> <code>List[str]</code> <p>A list of CSV datapoints.</p> required <p>Yields:</p> Type Description <code>TitanicData</code> <p>models.TitanicData: A titanic data object.</p> Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/transform.py</code> <pre><code>def titanic_transform(\n\trun_hash: str,\n\tdatapoints: List[str],\n) -&gt; Generator[models.TitanicData, None, None]:\n\t\"\"\"Generator that transforms a CSV datapoint into a titanic data object.\n\n\tArgs:\n\t    run_hash (str): The hash of the run.\n\t    datapoints (List[str]): A list of CSV datapoints.\n\n\tYields:\n\t    models.TitanicData: A titanic data object.\n\t\"\"\"\n\tfor datapoint in datapoints:\n\t\tdatapoint_decoded = list(csv.reader([datapoint]))[0]\n\n\t\t# Zip the headers and values together into a dictionary\n\t\tdatapoint_obj = models.TitanicData(\n\t\t\trun_hash=run_hash,\n\t\t\tPassengerId=datapoint_decoded[0] if datapoint_decoded[0] else '',\n\t\t\tSurvived=datapoint_decoded[1] if datapoint_decoded[1] else '',\n\t\t\tPclass=datapoint_decoded[2] if datapoint_decoded[2] else '',\n\t\t\tName=datapoint_decoded[3] if datapoint_decoded[3] else '',\n\t\t\tSex=datapoint_decoded[4] if datapoint_decoded[4] else '',\n\t\t\tAge=datapoint_decoded[5] if datapoint_decoded[5] else '',\n\t\t\tSibSp=datapoint_decoded[6] if datapoint_decoded[6] else '',\n\t\t\tParch=datapoint_decoded[7] if datapoint_decoded[7] else '',\n\t\t\tTicket=datapoint_decoded[8] if datapoint_decoded[8] else '',\n\t\t\tFare=datapoint_decoded[9] if datapoint_decoded[9] else '',\n\t\t\tCabin=datapoint_decoded[10] if datapoint_decoded[10] else '',\n\t\t\tEmbarked=datapoint_decoded[11] if datapoint_decoded[11] else '',\n\t\t)\n\n\t\t# Yield the datapoint object\n\t\tyield datapoint_obj\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.models","title":"<code>models</code>","text":"<p>Models for the ingest_data function. Simplifies type hinting.</p>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.models.EnvVars","title":"<code>EnvVars</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>A named tuple representing environment variables required for data ingestion.</p> <p>Attributes:</p> Name Type Description <code>gcp_project_id</code> <code>str</code> <p>The ID of the Google Cloud Platform project.</p> <code>bq_table_fqn</code> <code>str</code> <p>The fully-qualified name of the BigQuery table.</p> <code>topic_ingestion_complete</code> <code>str</code> <p>The name of the Pub/Sub topic for ingestion completion notifications.</p> Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/models.py</code> <pre><code>class EnvVars(NamedTuple):\n\t\"\"\"A named tuple representing environment variables required for data ingestion.\n\n\tAttributes:\n\t    gcp_project_id (str): The ID of the Google Cloud Platform project.\n\t    bq_table_fqn (str): The fully-qualified name of the BigQuery table.\n\t    topic_ingestion_complete (str): The name of the Pub/Sub topic for ingestion completion notifications.\n\t\"\"\"\n\n\tgcp_project_id: str\n\tbq_table_fqn: str\n\ttopic_ingestion_complete: str\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.models.GCPClients","title":"<code>GCPClients</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>A named tuple that contains GCP client objects for Storage, BigQuery, and Pub/Sub.</p> <p>Attributes:</p> Name Type Description <code>storage_client</code> <code>Client</code> <p>A client object for Google Cloud Storage.</p> <code>bigquery_client</code> <code>Client</code> <p>A client object for Google BigQuery.</p> <code>publisher</code> <code>PublisherClient</code> <p>A client object for Google Cloud Pub/Sub.</p> Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/models.py</code> <pre><code>class GCPClients(NamedTuple):\n\t\"\"\"A named tuple that contains GCP client objects for Storage, BigQuery, and Pub/Sub.\n\n\tAttributes:\n\t    storage_client (google.cloud.storage.Client): A client object for Google Cloud Storage.\n\t    bigquery_client (google.cloud.bigquery.Client): A client object for Google BigQuery.\n\t    publisher (google.cloud.pubsub_v1.PublisherClient): A client object for Google Cloud Pub/Sub.\n\t\"\"\"\n\n\tstorage_client: storage.Client\n\tbigquery_client: bigquery.Client\n\tpublisher: pubsub.PublisherClient\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.models.TitanicData","title":"<code>TitanicData</code>  <code>dataclass</code>","text":"<p>A class representing the data for the titanic dataset.</p> <p>Attributes:</p> Name Type Description <code>run_hash</code> <code>str</code> <p>The hash of the run.</p> <code>PassengerId</code> <code>Optional[str]</code> <p>The ID of the passenger.</p> <code>Survived</code> <code>Optional[str]</code> <p>Whether the passenger survived or not.</p> <code>Pclass</code> <code>Optional[str]</code> <p>The class of the passenger's ticket.</p> <code>Name</code> <code>Optional[str]</code> <p>The name of the passenger.</p> <code>Sex</code> <code>Optional[str]</code> <p>The gender of the passenger.</p> <code>Age</code> <code>Optional[str]</code> <p>The age of the passenger.</p> <code>SibSp</code> <code>Optional[str]</code> <p>The number of siblings/spouses aboard the Titanic.</p> <code>Parch</code> <code>Optional[str]</code> <p>The number of parents/children aboard the Titanic.</p> <code>Ticket</code> <code>Optional[str]</code> <p>The ticket number of the passenger.</p> <code>Fare</code> <code>Optional[str]</code> <p>The fare paid by the passenger.</p> <code>Cabin</code> <code>Optional[str]</code> <p>The cabin number of the passenger.</p> <code>Embarked</code> <code>Optional[str]</code> <p>The port of embarkation of the passenger.</p> Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/models.py</code> <pre><code>@dataclass(kw_only=True, frozen=True)\nclass TitanicData:\n\t\"\"\"A class representing the data for the titanic dataset.\n\n\tAttributes:\n\t\trun_hash (str): The hash of the run.\n\t\tPassengerId (Optional[str]): The ID of the passenger.\n\t\tSurvived (Optional[str]): Whether the passenger survived or not.\n\t\tPclass (Optional[str]): The class of the passenger's ticket.\n\t\tName (Optional[str]): The name of the passenger.\n\t\tSex (Optional[str]): The gender of the passenger.\n\t\tAge (Optional[str]): The age of the passenger.\n\t\tSibSp (Optional[str]): The number of siblings/spouses aboard the Titanic.\n\t\tParch (Optional[str]): The number of parents/children aboard the Titanic.\n\t\tTicket (Optional[str]): The ticket number of the passenger.\n\t\tFare (Optional[str]): The fare paid by the passenger.\n\t\tCabin (Optional[str]): The cabin number of the passenger.\n\t\tEmbarked (Optional[str]): The port of embarkation of the passenger.\n\t\"\"\"\n\n\trun_hash: str\n\tPassengerId: str | None\n\tSurvived: str | None = None\n\tPclass: str | None = None\n\tName: str | None\n\tSex: str | None = None\n\tAge: str | None = None\n\tSibSp: str | None = None\n\tParch: str | None = None\n\tTicket: str | None = None\n\tFare: str | None = None\n\tCabin: str | None = None\n\tEmbarked: str | None = None\n\n\t@classmethod\n\tdef from_dict(cls, data: Dict[str, Any]) -&gt; 'TitanicData':\n\t\t\"\"\"Creates a new instance of the TitanicData class from a dictionary.\n\n\t\tArgs:\n\t\t    data (Dict[str, Any]): A dictionary containing the data for a single passenger.\n\n\t\tReturns:\n\t\t    TitanicData: A new instance of the TitanicData class.\n\t\t\"\"\"\n\t\t# Convert the Survived column to a boolean\n\t\treturn cls(**data)\n\n\tdef to_dict(self) -&gt; Dict[str, Any]:\n\t\t\"\"\"Converts the TitanicData instance to a dictionary.\n\n\t\tReturns:\n\t\t    Dict[str, Any]: A dictionary containing the data for a single passenger.\n\t\t\"\"\"\n\t\treturn {\n\t\t\t'run_hash': self.run_hash,\n\t\t\t'PassengerId': self.PassengerId,\n\t\t\t'Survived': self.Survived,\n\t\t\t'Pclass': self.Pclass,\n\t\t\t'Name': self.Name,\n\t\t\t'Sex': self.Sex,\n\t\t\t'Age': self.Age,\n\t\t\t'SibSp': self.SibSp,\n\t\t\t'Parch': self.Parch,\n\t\t\t'Ticket': self.Ticket,\n\t\t\t'Fare': self.Fare,\n\t\t\t'Cabin': self.Cabin,\n\t\t\t'Embarked': self.Embarked,\n\t\t}\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.models.TitanicData.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Creates a new instance of the TitanicData class from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>A dictionary containing the data for a single passenger.</p> required <p>Returns:</p> Name Type Description <code>TitanicData</code> <code>TitanicData</code> <p>A new instance of the TitanicData class.</p> Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/models.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any]) -&gt; 'TitanicData':\n\t\"\"\"Creates a new instance of the TitanicData class from a dictionary.\n\n\tArgs:\n\t    data (Dict[str, Any]): A dictionary containing the data for a single passenger.\n\n\tReturns:\n\t    TitanicData: A new instance of the TitanicData class.\n\t\"\"\"\n\t# Convert the Survived column to a boolean\n\treturn cls(**data)\n</code></pre>"},{"location":"exercises/simple_mlops/step1/#mlops_usecase.a_ingest_data.app.funcs.models.TitanicData.to_dict","title":"<code>to_dict()</code>","text":"<p>Converts the TitanicData instance to a dictionary.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: A dictionary containing the data for a single passenger.</p> Source code in <code>functions/mlops_usecase/a_ingest_data/app/funcs/models.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n\t\"\"\"Converts the TitanicData instance to a dictionary.\n\n\tReturns:\n\t    Dict[str, Any]: A dictionary containing the data for a single passenger.\n\t\"\"\"\n\treturn {\n\t\t'run_hash': self.run_hash,\n\t\t'PassengerId': self.PassengerId,\n\t\t'Survived': self.Survived,\n\t\t'Pclass': self.Pclass,\n\t\t'Name': self.Name,\n\t\t'Sex': self.Sex,\n\t\t'Age': self.Age,\n\t\t'SibSp': self.SibSp,\n\t\t'Parch': self.Parch,\n\t\t'Ticket': self.Ticket,\n\t\t'Fare': self.Fare,\n\t\t'Cabin': self.Cabin,\n\t\t'Embarked': self.Embarked,\n\t}\n</code></pre>"},{"location":"exercises/simple_mlops/step2/","title":"Query Staging to Facts","text":"<ul> <li>Query Staging to Facts</li> <li>Introduction</li> <li>Tasks</li> <li>Create the Google Cloud Resources<ul> <li>1. Create a BigQuery Table</li> <li>2. Create the pubsub topic for update facts complete</li> </ul> </li> <li>Update the Cloud Function Code</li> <li>Deploy the cloud function</li> <li>Verify the data was moved from raw to facts</li> <li>Documentation</li> </ul>"},{"location":"exercises/simple_mlops/step2/#introduction","title":"Introduction","text":"<p>In this exercise, we will create the <code>Query To Facts</code> Cloud Function, that will perform the following tasks:</p> <ol> <li> <p>Will be triggered by the topic <code>[yourname]-ingestion-complete</code>.</p> </li> <li> <p>It will request a query to be executed by BigQuery. This query is partially done, and is part of the exercise to complete it.</p> </li> <li> <p>After successfully executing the query, this function will send a message to the topic <code>[yourname]-update-facts-complete</code>.</p> </li> </ol> <p>The Cloud Function <code>Update Facts</code> will utilize the BigQuery, and Pub/Sub client libraries for these tasks.</p> <p>The resources needed these tasks are:</p> <ul> <li>The already created Dataset and Raw Table in step 1.</li> <li>One Bigquery table, <code>Titanic Facts</code></li> <li>The table schema is at: <code>./resources/mlops_usecase/bigquery/facts_titanic_schema.json</code></li> <li>Two Pub/Sub topics, the one already created, and one named <code>[yourname]-update-facts-complete</code>, to where the function will send a message once complete.</li> </ul> <p>The outline of the Cloud Function code is available at <code>functions/mlops_usecase/b_update_facts/app</code>.</p> <pre><code>.\n\u2514\u2500\u2500 b_update_facts/\n    \u251c\u2500\u2500 app/\n    \u2502   \u251c\u2500\u2500 funcs/\n    \u2502   \u2502   \u251c\u2500\u2500 models.py # Models to make typechecking easier.\n    \u2502   \u2502   \u251c\u2500\u2500 gcp_apis.py # Functions to call google services.\n    \u2502   \u2502   \u2514\u2500\u2500 common.py # Common functions (Utils).\n    \u2502   \u251c\u2500\u2500 main.py # Main module and entry point for the Cloud Function\n    \u2502   \u2514\u2500\u2500 requirements.txt # Requirements for the function execution.\n    \u251c\u2500\u2500 config/\n    \u2502   \u2514\u2500\u2500 dev.env.yaml # Environment variables that will ship with the function deployment\n    \u2514\u2500\u2500 tests/\n        \u2514\u2500\u2500 test_*.py # Unit tests.\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#tasks","title":"Tasks","text":"<ul> <li> Create the Google Cloud Resources</li> <li> Update the Cloud Function Code</li> <li> Test the Cloud Function</li> <li> Deploy the Cloud Function</li> </ul>"},{"location":"exercises/simple_mlops/step2/#create-the-google-cloud-resources","title":"Create the Google Cloud Resources","text":"<p>Here are the resources necessary to complete the exercise:</p> <p>You can create the resources with Cloud Shell or in the Console. The end result will be the same. When creating a resource, choose either to create it with the cloud shell or the console, but not both.</p> <p>For Cloud Shell, set these variables:</p> <pre><code>export PROJECT_ID=$(gcloud config get-value project)\nexport PROJECT_NAME=$(gcloud config get-value project)\nexport PROJECT_NUMBER=$(gcloud projects describe $PROJECT_ID --format='value(projectNumber)')\nexport REGION=europe-west3\nexport YOURNAME=your_name_in_lowercase\n</code></pre> <p></p>"},{"location":"exercises/simple_mlops/step2/#1-create-a-bigquery-table","title":"1. Create a BigQuery Table","text":"<p>Create with either Cloud Shell OR the Console UI.</p> <p>With the console:</p> <p>Same as step 1, but now create a table with the name <code>titanic_facts</code> and use the schema <code>facts_titanic_schema.json</code></p> <p>With Cloud Shell, execute the following command:</p> <pre><code>bq mk \\\n    --project_id ${PROJECT_ID} \\\n    --table \\\n    --description \"Facts table for the Titanic dataset\" \\\n    --label=owner:${YOURNAME} \\\n    --label=project:${PROJECT_NAME} \\\n    --label=purpose:academy \\\n    --label=dataset:titanic \\\n    ${YOURNAME}_titanic.titanic_facts \\\n    ./resources/mlops_usecase/bigquery/facts_titanic_schema.json\n</code></pre> <p>Reference: bq mk --table</p>"},{"location":"exercises/simple_mlops/step2/#2-create-the-pubsub-topic-for-update-facts-complete","title":"2. Create the pubsub topic for update facts complete","text":"<p>With the Cloud Console:</p> <p>Same as in step 1, but now with the name <code>[yourname]-update-facts-complete</code></p> <p>Now we are ready to move to the cloud function code.</p> <p>With Cloud Shell:</p> <pre><code>gcloud pubsub topics create ${YOURNAME}-update-facts-complete \\\n    --project=${PROJECT_ID} \\\n    --labels=owner=${YOURNAME},project=${PROJECT_NAME},purpose=academy\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#update-the-cloud-function-code","title":"Update the Cloud Function Code","text":"<p>Here are the steps necessary to complete the exercise:</p> <ol> <li> <p>Set Environment Variables</p> <p>In the <code>b_update_facts/config/dev.env.yaml</code> file, change the environment variables for the correct ones.</p> <pre><code>##############################\n# 1. Environment variables ###\n##############################\n</code></pre> <pre><code>_GCP_PROJECT_ID: \"The GCP project ID where the resources are located\"\n_BIGQUERY_DATASET_ID: \"The BigQuery dataset ID you created\"\n_BIGQUERY_FACTS_TABLE_ID: \"The BigQuery staging table ID\"\n_BIGQUERY_STAGING_TABLE_ID: \"The BigQuery facts table ID where the data will be moved towards\"\n_TOPIC_UPDATE_FACTS_COMPLETE: \"The Pub/Sub topic ID where you will send a message once the data is ingested\"\n</code></pre> </li> <li> <p>Update the SQL Code</p> <p>Go to the file <code>b_update_facts/app/resources/staging_to_facts.sql</code>, and update the SQL Query to select the correct fields.</p> <pre><code>MERGE `{table_target}` AS T\nUSING (\n    SELECT\n        ???\n        IF(Survived = 1, True, False) AS Survived,\n        ????\n    FROM\n        `{table_source}`\n    WHERE run_hash = \"{run_hash}\"\n    QUALIFY ROW_NUMBER() OVER (PARTITION BY PassengerId ORDER BY Survived DESC) = 1\n) S\nON (\n    S.PassengerId = T.PassengerId\n)\nWHEN NOT MATCHED BY TARGET THEN\nINSERT (\n    ???,\n    ???,\n    ...\n)\nVALUES (\n    ???,\n    ???,\n    ...\n)\n</code></pre> </li> </ol>"},{"location":"exercises/simple_mlops/step2/#deploy-the-cloud-function","title":"Deploy the cloud function","text":"<p>You can check the deployment here in Cloud Build</p> <p>Reference: gcloud functions deploy</p> <pre><code># Remeber to have $YOURNAME from the first export to the Cloud Shell. \n# Uncomment the next lines if you see necessary\n# export REGION=europe-west3\n# export YOURNAME=your_name_in_lowercase\nexport FUNCTION_NAME=\"update_facts\"\nexport PATH_TO_FUNCTION=\"functions/mlops_usecase/b_update_facts\"\n\ngcloud beta functions deploy $YOURNAME-$FUNCTION_NAME \\\n    --gen2 --cpu=1 --memory=512MB \\\n    --region=europe-west3 \\\n    --runtime=python311 \\\n    --source=${PATH_TO_FUNCTION}/app/ \\\n    --env-vars-file=${PATH_TO_FUNCTION}/config/dev.env.yaml \\\n    --entry-point=main \\\n    --trigger-topic=${YOURNAME}-ingestion-complete\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#verify-the-data-was-moved-from-raw-to-facts","title":"Verify the data was moved from raw to facts","text":"<p>Warning</p> <p>Since the function is triggered by the <code>ingestion-complete</code> topic, you'll need to re-activate the previous ingestion function. You can achieve this by re-adding the <code>titanic.csv</code> file to the bucket. In the UI, you can upload and <code>Overwrite File</code>.</p> <p>In the command line, you can do this with the following command:</p> <p><code>gsutil cp resources/mlops_usecase/data/titanic.csv gs://${YOURNAME}-lz/</code></p> <p>To confirm that you sucessfully completed this phase, you can check the data in the BigQuery table.</p> <p></p> <p>And, the same way as in Step 1, you can also verify the logs of the Cloud Function <code>Update Facts</code>.</p>"},{"location":"exercises/simple_mlops/step2/#documentation","title":"Documentation","text":""},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.main","title":"<code>main</code>","text":"<p>This module contains a cloud function that updates a facts BigQuery table.</p> <p>With data from another table and publishes a message to a Pub/Sub topic.</p> <p>The function loads the necessary GCP clients, environment variables, and SQL query to update the BigQuery table. It then executes the query and publishes a message to the Pub/Sub topic.</p>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.main.load_clients","title":"<code>load_clients(gcp_project_id)</code>","text":"<p>Load the GCP clients.</p> <p>Parameters:</p> Name Type Description Default <code>gcp_project_id</code> <code>str</code> <p>The GCP project ID.</p> required <p>Returns:</p> Name Type Description <code>GCPClients</code> <code>GCPClients</code> <p>A tuple of GCP clients. With the following attributes:     bigquery_client: A bigquery client.     publisher: A pubsub publisher client.</p> Source code in <code>functions/mlops_usecase/b_update_facts/app/main.py</code> <pre><code>def load_clients(gcp_project_id: str) -&gt; models.GCPClients:\n\t\"\"\"Load the GCP clients.\n\n\tArgs:\n\t    gcp_project_id (str): The GCP project ID.\n\n\tReturns:\n\t    GCPClients: A tuple of GCP clients.\n\t        With the following attributes:\n\t            bigquery_client: A bigquery client.\n\t            publisher: A pubsub publisher client.\n\t\"\"\"\n\tbigquery_client = bigquery.Client(project=gcp_project_id)\n\tpublisher = pubsub.PublisherClient()\n\n\treturn models.GCPClients(bigquery_client=bigquery_client, publisher=publisher)\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.main.main","title":"<code>main(cloud_event)</code>","text":"<p>Entrypoint of the cloud function.</p> <p>Args: cloud_event (CloudEvent): The cloud event that triggered this function.</p> Source code in <code>functions/mlops_usecase/b_update_facts/app/main.py</code> <pre><code>@functions_framework.cloud_event\ndef main(cloud_event: CloudEvent) -&gt; None:\n\t\"\"\"Entrypoint of the cloud function.\n\n\tArgs:\n\tcloud_event (CloudEvent): The cloud event that triggered this function.\n\t\"\"\"\n\t# event_data = base64.b64decode(cloud_event.data['message']['data']).decode()\n\tevent_attributes = cloud_event.data['message']['attributes']\n\n\tif not hasattr(main, 'env_vars'):\n\t\tenv_vars = _env_vars()\n\n\tif not hasattr(main, 'gcp_clients'):\n\t\tgcp_clients = load_clients(gcp_project_id=env_vars.gcp_project_id)\n\n\tpath = Path('./resources/staging_to_facts.sql')\n\n\tquery = common.load_query(\n\t\ttable_facts=env_vars.bq_facts_table_fqn,\n\t\ttable_raw=env_vars.bq_staging_table_fqn,\n\t\tquery_path=path,\n\t\trun_hash=event_attributes['closer-run-hash'],\n\t)\n\n\t_ = gcp_apis.execute_query_result(\n\t\tBQ=gcp_clients.bigquery_client,\n\t\tquery=query,\n\t)\n\n\tgcp_apis.pubsub_publish_message(\n\t\tPS=gcp_clients.publisher,  # type: ignore\n\t\tproject_id=env_vars.gcp_project_id,  # type: ignore\n\t\ttopic_id=env_vars.topic_update_facts_complete,  # type: ignore\n\t\tmessage=json.dumps({'message': 'I finished passing the staging data to facts', 'training_data_table': env_vars.bq_facts_table_fqn}),\n\t\tattributes={'train_model': 'True', 'dataset': 'titanic'},\n\t)\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.funcs.gcp_apis","title":"<code>gcp_apis</code>","text":"<p>This module contains the functions that interact with the GCP APIs.</p>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.funcs.gcp_apis.execute_query","title":"<code>execute_query(BQ, job_config, query, location=None, job_id=None)</code>","text":"<p>Execute a query with the desired job configuration.</p> <p>Parameters:</p> Name Type Description Default <code>BQ</code> <code>Client</code> <p>The BigQuery client instance.</p> required <code>job_config</code> <code>QueryJobConfig</code> <p>The desired job configuration for the query execution.</p> required <code>query</code> <code>str</code> <p>The query string to be executed.</p> required <code>location</code> <code>str</code> <p>The location where the query will be run. Defaults to None.</p> <code>None</code> <code>job_id</code> <code>str</code> <p>The ID of the job. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>QueryJob</code> <p>bigquery.QueryJob: A new query job instance.</p> Source code in <code>functions/mlops_usecase/b_update_facts/app/funcs/gcp_apis.py</code> <pre><code>def execute_query(\n    BQ: bigquery.Client,\n    job_config: bigquery.QueryJobConfig,\n    query: str,\n    location: str | None = None,\n    job_id: str | None = None,\n) -&gt; bigquery.QueryJob:\n    \"\"\"Execute a query with the desired job configuration.\n\n    Args:\n        BQ (bigquery.Client): The BigQuery client instance.\n        job_config (bigquery.QueryJobConfig): The desired job configuration for the query execution.\n        query (str): The query string to be executed.\n        location (str, optional): The location where the query will be run. Defaults to None.\n        job_id (str, optional): The ID of the job. Defaults to None.\n\n    Returns:\n        bigquery.QueryJob: A new query job instance.\n    \"\"\"\n    return BQ.query(\n        query,\n        job_config=job_config,\n        job_id=job_id,\n        location=location,\n    )\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.funcs.gcp_apis.execute_query_result","title":"<code>execute_query_result(BQ, query, job_config=bigquery.QueryJobConfig(dry_run=False, use_query_cache=True), job_id=None, location=None)</code>","text":"<p>Executes a BigQuery query and returns the results.</p> <p>This function is an intermediate step between executing a query and holding the query results. It passes the default job configuration to the execute_query function.</p> <p>Parameters:</p> Name Type Description Default <code>BQ</code> <code>Client</code> <p>A BigQuery client object.</p> required <code>query</code> <code>str</code> <p>The query to execute.</p> required <code>job_config</code> <code>QueryJobConfig</code> <p>The configuration for the query job. Defaults to a QueryJobConfig object with dry_run=False and use_query_cache=True.</p> <code>QueryJobConfig(dry_run=False, use_query_cache=True)</code> <code>job_id</code> <code>str</code> <p>The ID of the job. Defaults to None.</p> <code>None</code> <code>location</code> <code>str</code> <p>The location of the job. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The results of the query execution.</p> Source code in <code>functions/mlops_usecase/b_update_facts/app/funcs/gcp_apis.py</code> <pre><code>def execute_query_result(\n    BQ: bigquery.Client,\n    query: str,\n    job_config: bigquery.QueryJobConfig = bigquery.QueryJobConfig(\n        dry_run=False, use_query_cache=True),\n    job_id: str | None = None,\n    location: str | None = None,\n) -&gt; Any:\n    \"\"\"Executes a BigQuery query and returns the results.\n\n    This function is an intermediate step between executing a query and\n    holding the query results. It passes the default job configuration to\n    the execute_query function.\n\n    Args:\n        BQ (google.cloud.bigquery.client.Client): A BigQuery client object.\n        query (str): The query to execute.\n        job_config (google.cloud.bigquery.job.QueryJobConfig, optional):\n            The configuration for the query job. Defaults to a QueryJobConfig\n            object with dry_run=False and use_query_cache=True.\n        job_id (str, optional): The ID of the job. Defaults to None.\n        location (str, optional): The location of the job. Defaults to None.\n\n    Returns:\n        The results of the query execution.\n    \"\"\"\n    return execute_query(\n        BQ=BQ,\n        query=query,\n        job_config=job_config,\n        job_id=job_id,\n        location=location,\n    ).result()\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.funcs.gcp_apis.pubsub_publish_message","title":"<code>pubsub_publish_message(PS, project_id, topic_id, message, attributes={})</code>","text":"<p>Publishes a message to a Pub/Sub topic.</p> <p>Parameters:</p> Name Type Description Default <code>PS</code> <code>PublisherClient</code> <p>The pubsub client.</p> required <code>project_id</code> <code>str</code> <p>The ID of the project where the topic is located.</p> required <code>topic_id</code> <code>str</code> <p>The ID of the topic.</p> required <code>message</code> <code>str</code> <p>The message to publish.</p> required <code>attributes</code> <code>Dict[str, str]</code> <p>The attributes of the message. Defaults to {}.</p> <code>{}</code> Source code in <code>functions/mlops_usecase/b_update_facts/app/funcs/gcp_apis.py</code> <pre><code>def pubsub_publish_message(\n    PS: pubsub.PublisherClient,\n    project_id: str,\n    topic_id: str,\n    message: str,\n    attributes: Dict[str, str] = {},\n) -&gt; None:\n    \"\"\"Publishes a message to a Pub/Sub topic.\n\n    Args:\n        PS (pubsub.PublisherClient): The pubsub client.\n        project_id (str): The ID of the project where the topic is located.\n        topic_id (str): The ID of the topic.\n        message (str): The message to publish.\n        attributes (Dict[str, str], optional): The attributes of the message.\n            Defaults to {}.\n    \"\"\"\n    _topic = PS.topic_path(project_id, topic_id)\n\n    PS.publish(\n        topic=_topic,\n        data=message.encode('utf-8'),\n        **attributes,)\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.funcs.common","title":"<code>common</code>","text":"<p>Common functions for the train_model pipeline.</p>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.funcs.common.load_query","title":"<code>load_query(table_facts, table_raw, query_path, run_hash)</code>","text":"<p>Inserts raw data into a temporary table. Common pattern in our ETL pipelines.</p> <p>This function uses the function file_contents to call the appropriate SQL query and formats it with this function parameters.</p> <p>Parameters:</p> Name Type Description Default <code>table_facts</code> <code>str</code> <p>The fqn of the facts table in BigQuery.</p> required <code>table_raw</code> <code>str</code> <p>The fqn of the raw table in BigQuery.</p> required <code>query_path</code> <code>Path</code> <p>The path to the SQL query script.</p> required <code>run_hash</code> <code>str</code> <p>A unique identifier for the run.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string with the query built based on the args.</p> <code>str</code> <p>This query can be executed later.</p> Source code in <code>functions/mlops_usecase/b_update_facts/app/funcs/common.py</code> <pre><code>def load_query(\n    table_facts: str,\n    table_raw: str,\n    query_path: Path,\n    run_hash: str,\n) -&gt; str:\n    \"\"\"Inserts raw data into a temporary table. Common pattern in our ETL pipelines.\n\n    This function uses the function file_contents to call the appropriate\n    SQL query and formats it with this function parameters.\n\n    Args:\n        table_facts (str): The fqn of the facts table in BigQuery.\n        table_raw (str): The fqn of the raw table in BigQuery.\n        query_path (Path): The path to the SQL query script.\n        run_hash (str): A unique identifier for the run.\n\n    Returns:\n        str: A string with the query built based on the args.\n        This query can be executed later.\n    \"\"\"\n    query: str = file_contents(\n        path=query_path\n    ).format(\n        table_source=table_raw,\n        table_target=table_facts,\n        run_hash=run_hash,\n    )\n    return query\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.funcs.models","title":"<code>models</code>","text":"<p>This module contains named tuples for the update_facts function.</p> <p>The module contains two named tuples: GCPClients and EnvVars. GCPClients is a named tuple containing instances of Google Cloud Platform clients for interacting with BigQuery and Pub/Sub. EnvVars is a named tuple containing environment variables required for the update_facts function.</p>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.funcs.models.EnvVars","title":"<code>EnvVars</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>NamedTuple containing environment variables required for the update_facts function.</p> <p>Attributes:</p> Name Type Description <code>gcp_project_id</code> <code>str</code> <p>The ID of the Google Cloud Platform project.</p> <code>bq_facts_table_fqn</code> <code>str</code> <p>The fully-qualified name of the BigQuery table containing facts data.</p> <code>bq_staging_table_fqn</code> <code>str</code> <p>The fully-qualified name of the BigQuery table containing staging data.</p> <code>topic_update_facts_complete</code> <code>str</code> <p>The name of the Pub/Sub topic to publish a message to when the update_facts function completes.</p> Source code in <code>functions/mlops_usecase/b_update_facts/app/funcs/models.py</code> <pre><code>class EnvVars(NamedTuple):\n    \"\"\"NamedTuple containing environment variables required for the update_facts function.\n\n    Attributes:\n        gcp_project_id (str): The ID of the Google Cloud Platform project.\n        bq_facts_table_fqn (str): The fully-qualified name of the BigQuery table containing facts data.\n        bq_staging_table_fqn (str): The fully-qualified name of the BigQuery table containing staging data.\n        topic_update_facts_complete (str): The name of the Pub/Sub topic to publish a message to when the update_facts function completes.\n    \"\"\"\n    gcp_project_id: str\n    bq_facts_table_fqn: str\n    bq_staging_table_fqn: str\n    topic_update_facts_complete: str\n</code></pre>"},{"location":"exercises/simple_mlops/step2/#mlops_usecase.b_update_facts.app.funcs.models.GCPClients","title":"<code>GCPClients</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>A named tuple containing GCP client instances.</p> <p>Attributes:</p> Name Type Description <code>bigquery_client</code> <code>Client</code> <p>A client for interacting with BigQuery.</p> <code>publisher</code> <code>PublisherClient</code> <p>A client for interacting with Pub/Sub.</p> Source code in <code>functions/mlops_usecase/b_update_facts/app/funcs/models.py</code> <pre><code>class GCPClients(NamedTuple):\n    \"\"\"A named tuple containing GCP client instances.\n\n    Attributes:\n        bigquery_client (google.cloud.bigquery.client.Client): A client for interacting with BigQuery.\n        publisher (google.cloud.pubsub_v1.PublisherClient): A client for interacting with Pub/Sub.\n    \"\"\"\n    bigquery_client: bigquery.Client\n    publisher: pubsub.PublisherClient\n</code></pre>"},{"location":"exercises/simple_mlops/step3/","title":"Deploy a Cloud function that trains a model and saves it in GCS","text":"<ul> <li>Deploy a Cloud function that trains a model and saves it in GCS</li> <li>Introduction</li> <li>Tasks</li> <li>Create the Google Cloud Resources<ul> <li>1. Create the models GCS Bucket</li> <li>2. Create the pubsub topic for train model complete</li> </ul> </li> <li>Update the Cloud Function Code</li> <li>Deploy the cloud function</li> <li>Confirm the model was uplodad</li> <li>Documentation</li> </ul>"},{"location":"exercises/simple_mlops/step3/#introduction","title":"Introduction","text":"<p>In this exercise, we will create a Cloud Function called <code>Train Model</code>, which will be responsible for training a machine learning model using the data ingested in the previous steps. The function will be triggered by the <code>update-facts-complete</code> Pub/Sub topic, ensuring it starts training once new facts data is available in the BigQuery table. The steps involved in this process are as follows:</p> <ol> <li> <p>The <code>Train Model</code> Cloud Function is subscribed to the <code>[yourname]-update-facts-complete</code> topic, and it will be triggered automatically when a new message is published, indicating that new data has been loaded into the BigQuery table.</p> </li> <li> <p>Upon being triggered, the <code>train_model</code> function retrieves the data from the <code>Titanic Facts</code> BigQuery table using the appropriate query. This data will be used to train a machine learning model, such as an Scikit-learn Random Forest or Logistic Regression model.</p> </li> <li> <p>After the model is trained using the fetched data, the <code>Train Model</code> function saves the trained model to the <code>[yourname]-models</code> Google Cloud Storage bucket. You can choose the name for this model, but it should be unique.</p> </li> </ol> <p>This exercise will guide you through the process of developing the <code>train_model</code> Cloud Function, which leverages the power of BigQuery, Scikit-learn, and Google Cloud Storage to create, train, and store a machine learning model.</p> <p>For this you will need these resources:</p> <ul> <li>The already created Data Set in step 1.</li> <li>The already created Bigquery Table in step 2.</li> <li>A Pub/Sub topic named <code>[yourname]-train-model-complete</code> where you will publish a success message.</li> <li>One GCS Bucket named <code>[yourname]-models</code> where you will save the model</li> </ul> <p>The outline of the Cloud Function code is available at <code>./functions/simple_mlops/c_train_model/app</code></p> <pre><code>c_train_model/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 funcs/\n\u2502   \u2502   \u251c\u2500\u2500 models.py # Models to make typechecking easier.\n\u2502   \u2502   \u251c\u2500\u2500 gcp_apis.py # Functions to call google services.\n\u2502   \u2502   \u251c\u2500\u2500 common.py # Common functions (Utils).\n|   |   \u2514\u2500\u2500 train_model.py # Train model functions\n\u2502   \u251c\u2500\u2500 main.py # Main module and entry point for the Cloud Function\n\u2502   \u2514\u2500\u2500 requirements.txt # Requirements for the function execution.\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 dev.env.yaml # Environment variables that will ship with the function deployment\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_*.py # Unit tests.\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#tasks","title":"Tasks","text":"<ul> <li> Create the Google Cloud Resources</li> <li> Update the Cloud Function Code</li> <li> Test the Cloud Function</li> <li> Deploy the Cloud Function</li> </ul>"},{"location":"exercises/simple_mlops/step3/#create-the-google-cloud-resources","title":"Create the Google Cloud Resources","text":"<p>Here are the resources necessary to complete the exercise:</p> <p>You can create the resources with Cloud Shell or in the Console. The end result will be the same. When creating a resource, choose either to create it with the cloud shell or the console, but not both.</p> <p>For Cloud Shell, set these variables:</p> <pre><code>export PROJECT_ID=$(gcloud config get-value project)\nexport PROJECT_NAME=$(gcloud config get-value project)\nexport REGION=europe-west3\nexport YOURNAME=your_name_in_lowercase\n</code></pre> <p></p>"},{"location":"exercises/simple_mlops/step3/#1-create-the-models-gcs-bucket","title":"1. Create the models GCS Bucket","text":"<p>Create with either Cloud Shell OR the Console UI.</p> <p>With the console:</p> <p>Same as in step 1, but now the bucket name is <code>[yourname]-models</code></p> <pre><code>gsutil mb \\\n    -c regional \\\n    -l ${REGION} \\\n    -p ${PROJECT_NAME} \\\n    gs://${YOURNAME}-models\n\ngsutil label ch -l owner:${YOURNAME} gs://${YOURNAME}-models\ngsutil label ch -l project:${PROJECT_NAME} gs://${YOURNAME}-models\ngsutil label ch -l purpose:academy gs://${YOURNAME}-models\n</code></pre> <p>Reference: gsutil mb, gsutil label</p>"},{"location":"exercises/simple_mlops/step3/#2-create-the-pubsub-topic-for-train-model-complete","title":"2. Create the pubsub topic for train model complete","text":"<p>With the Cloud Console:</p> <p>Same as in step 1, but now with the name <code>[yourname]-train-model-complete</code></p> <p>Now we are ready to move to the cloud function code.</p> <p>With Cloud Shell:</p> <pre><code>gcloud pubsub topics create ${YOURNAME}-train-model-complete \\\n    --project=${PROJECT_ID} \\\n    --labels=owner=${YOURNAME},project=${PROJECT_NAME},purpose=academy\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#update-the-cloud-function-code","title":"Update the Cloud Function Code","text":"<ol> <li> <p>Set Environment Variables</p> <p>In the <code>c_train_model/config/dev.env.yaml</code> file, change the environment variables for the correct ones.</p> <pre><code>##############################\n# 1. Environment variables ###\n##############################\n</code></pre> <pre><code>_GCP_PROJECT_ID: \"The GCP project ID where the resources are located\"\n_GCS_BUCKET_NAME_MODELS: \"The GCS bucket name where the models will be saved\"\n_TOPIC_TRAINING_COMPLETE: \"The Pub/Sub topic name where the success message will be published\"\n</code></pre> </li> </ol>"},{"location":"exercises/simple_mlops/step3/#deploy-the-cloud-function","title":"Deploy the cloud function","text":"<p>You can check the deployment here in Cloud Build</p> <p>Reference: gcloud functions deploy</p> <pre><code># Remeber to have $YOURNAME from the first export to the Cloud Shell. \n# Uncomment the next lines if you see necessary\n# export REGION=europe-west3\n# export YOURNAME=your_name_in_lowercase\nexport FUNCTION_NAME=\"train_model\"\nexport PATH_TO_FUNCTION=\"functions/mlops_usecase/c_train_model\"\n\ngcloud beta functions deploy $YOURNAME-$FUNCTION_NAME \\\n    --gen2 --cpu=1 --memory=512MB \\\n    --region=europe-west3 \\\n    --runtime=python311 \\\n    --source=${PATH_TO_FUNCTION}/app/ \\\n    --env-vars-file=${PATH_TO_FUNCTION}/config/dev.env.yaml \\\n    --entry-point=main \\\n    --trigger-topic=$YOURNAME-update-facts-complete\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#confirm-the-model-was-uplodad","title":"Confirm the model was uplodad","text":"<p>Warning</p> <p>Since the function is triggered by the <code>update-facts-complete</code>, which is activated by CF <code>Update Facts</code>, which is activated by the <code>ingestion-complete</code> topic, you'll need to re-activate the previous ingestion function. You can achieve this by re-adding the <code>titanic.csv</code>. file to the bucket. In the UI, you can upload and <code>Overwrite File</code>.</p> <p>In the command line, you can do this with the following command:</p> <p><code>gsutil cp resources/mlops_usecase/data/titanic.csv gs://${YOURNAME}-lz/</code></p> <p>To verify that the model was correctly uploaded, you can 1) Check the cloud function logs, and 2) Go to your cloud storage bucket <code>your_name_in_lowercase-models</code>.</p> <p>1.     </p> <p>2.     </p>"},{"location":"exercises/simple_mlops/step3/#documentation","title":"Documentation","text":""},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.main","title":"<code>main</code>","text":""},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.main.load_clients","title":"<code>load_clients(gcp_project_id)</code>","text":"<p>Load the GCP clients.</p> <p>Parameters:</p> Name Type Description Default <code>gcp_project_id</code> <code>str</code> <p>The GCP project ID.</p> required <p>Returns:</p> Name Type Description <code>GCPClients</code> <code>GCPClients</code> <p>A tuple of GCP clients. With the following attributes:     storage_client: A storage client.     bigquery_client: A bigquery client.</p> Source code in <code>functions/mlops_usecase/c_train_model/app/main.py</code> <pre><code>def load_clients(gcp_project_id: str) -&gt; models.GCPClients:\n\t\"\"\"Load the GCP clients.\n\n\tArgs:\n\t    gcp_project_id (str): The GCP project ID.\n\n\tReturns:\n\t    GCPClients: A tuple of GCP clients.\n\t        With the following attributes:\n\t            storage_client: A storage client.\n\t            bigquery_client: A bigquery client.\n\t\"\"\"\n\tstorage_client = storage.Client(project=gcp_project_id)\n\tbigquery_client = bigquery.Client(project=gcp_project_id)\n\n\treturn models.GCPClients(storage_client=storage_client, bigquery_client=bigquery_client)\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.main.main","title":"<code>main(cloud_event)</code>","text":"<p>Entrypoint of the cloud function.</p> Source code in <code>functions/mlops_usecase/c_train_model/app/main.py</code> <pre><code>@functions_framework.cloud_event\ndef main(cloud_event: CloudEvent) -&gt; None:\n\t\"\"\"Entrypoint of the cloud function.\"\"\"\n\tif not hasattr(main, 'env_vars'):\n\t\tenv_vars = _env_vars()\n\n\tif not hasattr(main, 'gcp_clients'):\n\t\tgcp_clients = load_clients(gcp_project_id=env_vars.gcp_project_id)\n\n\tevent_message: dict = cloud_event.get_data()  # type: ignore\n\n\tdata = json.loads(common.decode_base64_to_string(event_message['message']['data']))\n\n\t# Train the model\n\tif event_message['message']['attributes']['train_model'] == 'True':\n\t\tpath = common.get_path_to_file()\n\n\t\tquery = common.query_train_data(\n\t\t\ttable_fqn=data['training_data_table'],  # type: ignore\n\t\t\tquery_path=path,\n\t\t)\n\t\tdf = gcp_apis.query_to_pandas_dataframe(\n\t\t\tquery=query,\n\t\t\tBQ=gcp_clients.bigquery_client,  # type: ignore\n\t\t)\n\n\t\tpipeline = train_models.titanic_train(\n\t\t\tdf=df,\n\t\t)\n\n\t\tgcp_apis.model_save_to_storage(\n\t\t\tCS=gcp_clients.storage_client,\n\t\t\tmodel=pipeline,\n\t\t\tbucket_name=env_vars.bucket_name,\n\t\t)\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.train_models","title":"<code>train_models</code>","text":""},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.train_models.titanic_train","title":"<code>titanic_train(df, classifier=RandomForestClassifier(n_estimators=100, random_state=42))</code>","text":"<p>Train a model into a pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Dataframe</code> <p>The dataframe with the data to train the model.</p> required <code>classifier</code> <code>Callable</code> <p>The classifier to use. Defaults to RandomForestClassifier(n_estimators=100, random_state=42).</p> <code>RandomForestClassifier(n_estimators=100, random_state=42)</code> Source code in <code>functions/mlops_usecase/c_train_model/app/funcs/train_models.py</code> <pre><code>def titanic_train(\n\tdf: pd.DataFrame,\n\tclassifier: ClassifierMixin = RandomForestClassifier(n_estimators=100, random_state=42),\n) -&gt; Pipeline:\n\t\"\"\"Train a model into a pipeline.\n\n\tArgs:\n\t    df (pd.Dataframe): The dataframe with the data to train the model.\n\t    classifier (Callable, optional): The classifier to use.\n\t        Defaults to RandomForestClassifier(n_estimators=100, random_state=42).\n\t\"\"\"\n\t# Preprocess the data\n\tX = df.drop(\n\t\tcolumns=['Survived', 'PassengerId', 'Name', 'Ticket', 'Cabin']\n\t)  # OPTIONAL [1]: add 'set_type' or other columns that shouldn't be passed to the model.\n\ty = df['Survived']\n\n\t# Drop rows with missing target values\n\tmissing_target_rows = y.isna()\n\tX = X[~missing_target_rows]\n\ty = y[~missing_target_rows]\n\n\t# Preprocessing for numerical columns\n\tnumeric_features = ['Age', 'SibSp', 'Parch', 'Fare']\n\tnumeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n\n\t# Preprocessing for the 'Sex' column\n\tsex_feature = ['Sex']\n\tsex_transformer = Pipeline(\n\t\tsteps=[('imputer', SimpleImputer(strategy='constant', fill_value='unknown')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]\n\t)\n\n\t# Preprocessing for the 'Embarked' column\n\tembarked_feature = ['Embarked']\n\tembarked_transformer = Pipeline(\n\t\tsteps=[('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore'))]\n\t)\n\n\t# Preprocessing for the 'Pclass' column\n\tpclass_feature = ['Pclass']\n\tpclass_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\n\t# Combine transformers into a preprocessor\n\tpreprocessor = ColumnTransformer(\n\t\ttransformers=[\n\t\t\t('num', numeric_transformer, numeric_features),\n\t\t\t('sex', sex_transformer, sex_feature),\n\t\t\t('embarked', embarked_transformer, embarked_feature),\n\t\t\t('pclass', pclass_transformer, pclass_feature),\n\t\t]\n\t)\n\n\t# Create the pipeline with preprocessing and the Random Forest classifier\n\tpipeline = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', classifier)])\n\n\t# Train the model on the training data\n\tpipeline.fit(X, y)\n\n\treturn pipeline\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.common","title":"<code>common</code>","text":"<p>Common functions for the update_facts pipeline.</p>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.common.decode_base64_to_string","title":"<code>decode_base64_to_string(base64_string)</code>","text":"<p>Decodes a base64 string to a string.</p> <p>Parameters:</p> Name Type Description Default <code>base64_string</code> <code>str</code> <p>A base64 string.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The decoded string.</p> Source code in <code>functions/mlops_usecase/c_train_model/app/funcs/common.py</code> <pre><code>def decode_base64_to_string(\n    base64_string: str,\n) -&gt; str:\n    \"\"\"Decodes a base64 string to a string.\n\n    Args:\n        base64_string (str): A base64 string.\n\n    Returns:\n        str: The decoded string.\n    \"\"\"\n    return base64.b64decode(base64_string).decode('utf-8')\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.common.file_contents","title":"<code>file_contents(path)</code>","text":"<p>Reads the contents of a file and returns it as a string.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the file to be read.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The contents of the file as a string.</p> Source code in <code>functions/mlops_usecase/c_train_model/app/funcs/common.py</code> <pre><code>def file_contents(path: Path) -&gt; str:\n    \"\"\"Reads the contents of a file and returns it as a string.\n\n    Args:\n        path (Path): The path to the file to be read.\n\n    Returns:\n        str: The contents of the file as a string.\n    \"\"\"\n    with open(path, 'r', encoding='utf-8') as f:\n        return f.read().replace('\\n', ' ')\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.common.query_train_data","title":"<code>query_train_data(table_fqn, query_path)</code>","text":"<p>Query to get training data from BigQuery.</p> <p>This function uses the function file_contents to call the appropriate SQL query and formats it with this function parameters.</p> <p>Parameters:</p> Name Type Description Default <code>table_fqn</code> <code>str</code> <p>The fully-qualified name of the table in BigQuery.</p> required <code>query_path</code> <code>Path</code> <p>The path to the SQL query script.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string with the query built based on the args.</p> <code>str</code> <p>This query can be executed later.</p> Source code in <code>functions/mlops_usecase/c_train_model/app/funcs/common.py</code> <pre><code>def query_train_data(\n    table_fqn: str,\n    query_path: Path,\n) -&gt; str:\n    \"\"\"Query to get training data from BigQuery.\n\n    This function uses the function file_contents to call the appropriate\n    SQL query and formats it with this function parameters.\n\n    Args:\n        table_fqn (str): The fully-qualified name of the table in BigQuery.\n        query_path (Path): The path to the SQL query script.\n\n    Returns:\n        str: A string with the query built based on the args.\n        This query can be executed later.\n    \"\"\"\n    query: str = file_contents(\n        path=query_path\n    ).format(\n        table_source=table_fqn,\n    )\n    return query\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.gcp_apis","title":"<code>gcp_apis</code>","text":""},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.gcp_apis.model_save_to_storage","title":"<code>model_save_to_storage(CS, bucket_name, model, model_name='nar-rayya', content_type='text/plain')</code>","text":"<p>Saves a machine learning model to Google Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>CS</code> <code>Client</code> <p>A Google Cloud Storage client object.</p> required <code>bucket_name</code> <code>str</code> <p>The name of the bucket to save the model to.</p> required <code>model</code> <code>Pipeline</code> <p>The machine learning model to save.</p> required <code>model_name</code> <code>str</code> <p>The name to give the saved model. Defaults to 'nar-rayya'.</p> <code>'nar-rayya'</code> <code>content_type</code> <code>str</code> <p>The content type of the saved model. Defaults to 'text/plain'.</p> <code>'text/plain'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>functions/mlops_usecase/c_train_model/app/funcs/gcp_apis.py</code> <pre><code>def model_save_to_storage(\n    CS: storage.Client,\n    bucket_name: str,\n    model: Pipeline,\n    model_name: str = 'nar-rayya',\n    content_type: str = 'text/plain',\n) -&gt; None:\n    \"\"\"Saves a machine learning model to Google Cloud Storage.\n\n    Args:\n        CS (google.cloud.storage.client.Client): A Google Cloud Storage client object.\n        bucket_name (str): The name of the bucket to save the model to.\n        model (sklearn.pipeline.Pipeline): The machine learning model to save.\n        model_name (str, optional): The name to give the saved model. Defaults to 'nar-rayya'.\n        content_type (str, optional): The content type of the saved model. Defaults to 'text/plain'.\n\n    Returns:\n        None\n    \"\"\"\n    # https://stackoverflow.com/questions/56880703/read-model-as-bytes-without-saving-in-location-in-python\n    # https://stackoverflow.com/questions/51921142/how-to-load-a-model-saved-in-joblib-file-from-google-cloud-storage-bucket\n    bytes_container = BytesIO()\n    joblib.dump(model, bytes_container)\n    bytes_container.seek(0)  # update to enable reading\n\n    _storage_write_bytes_file_to_bucket(\n        CS=CS,\n        bucket_name=bucket_name,\n        model_content=bytes_container.read(),\n        model_name=model_name,\n        content_type=content_type,\n    )\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.gcp_apis.query_to_pandas_dataframe","title":"<code>query_to_pandas_dataframe(query, BQ)</code>","text":"<p>This function takes a SQL query and a BigQuery client object as input, and returns the result of the query as a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The SQL query to execute.</p> required <code>BQ</code> <code>Client</code> <p>The BigQuery client object to use for executing the query.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The result of the query as a pandas DataFrame.</p> Source code in <code>functions/mlops_usecase/c_train_model/app/funcs/gcp_apis.py</code> <pre><code>def query_to_pandas_dataframe(\n    query: str,\n    BQ: bigquery.Client\n) -&gt; pd.DataFrame:\n    \"\"\"This function takes a SQL query and a BigQuery client object as input, and\n    returns the result of the query as a pandas DataFrame.\n\n    Args:\n        query (str): The SQL query to execute.\n        BQ (bigquery.Client): The BigQuery client object to use for executing the query.\n\n    Returns:\n        pd.DataFrame: The result of the query as a pandas DataFrame.\n    \"\"\"\n    return BQ.query(query).to_dataframe()\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.models","title":"<code>models</code>","text":"<p>Models for the ingest_data function. Simplifies type hinting.</p>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.models.EnvVars","title":"<code>EnvVars</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>A named tuple representing environment variables used in the model training process.</p> <p>Attributes:</p> Name Type Description <code>gcp_project_id</code> <code>str</code> <p>The ID of the Google Cloud Platform project.</p> <code>bucket_name</code> <code>str</code> <p>The name of the Google Cloud Storage bucket where the model artifacts will be stored.</p> <code>topic_training_complete</code> <code>str</code> <p>The name of the Pub/Sub topic to which a message is published when training is complete.</p> Source code in <code>functions/mlops_usecase/c_train_model/app/funcs/models.py</code> <pre><code>class EnvVars(NamedTuple):\n    \"\"\"A named tuple representing environment variables used in the model training process.\n\n    Attributes:\n        gcp_project_id (str): The ID of the Google Cloud Platform project.\n        bucket_name (str): The name of the Google Cloud Storage bucket where the model artifacts will be stored.\n        topic_training_complete (str): The name of the Pub/Sub topic to which a message is published when training is complete.\n    \"\"\"\n    gcp_project_id: str\n    bucket_name: str\n    topic_training_complete: str\n</code></pre>"},{"location":"exercises/simple_mlops/step3/#mlops_usecase.c_train_model.app.funcs.models.GCPClients","title":"<code>GCPClients</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>A named tuple that contains clients for Google Cloud Platform services.</p> <p>Attributes:</p> Name Type Description <code>storage_client</code> <code>Client</code> <p>A client for Google Cloud Storage.</p> <code>bigquery_client</code> <code>Client</code> <p>A client for Google BigQuery.</p> <code>publisher</code> <code>PublisherClient</code> <p>A client for Google Cloud Pub/Sub.</p> Source code in <code>functions/mlops_usecase/c_train_model/app/funcs/models.py</code> <pre><code>class GCPClients(NamedTuple):\n    \"\"\"A named tuple that contains clients for Google Cloud Platform services.\n\n    Attributes:\n        storage_client (google.cloud.storage.Client): A client for Google Cloud Storage.\n        bigquery_client (google.cloud.bigquery.Client): A client for Google BigQuery.\n        publisher (google.cloud.pubsub_v1.PublisherClient): A client for Google Cloud Pub/Sub.\n    \"\"\"\n    storage_client: storage.Client\n    bigquery_client: bigquery.Client\n</code></pre>"},{"location":"exercises/simple_mlops/step4/","title":"Create an endpoint to serve the model to the outside world","text":"<p>In this exercise, you'll be working with the <code>predictions_endpoint</code> Cloud Function. This HTTP-triggered function serves as the prediction endpoint for clients to send new data points. Upon receiving a request containing new data, the function performs the following steps:</p> <ol> <li>It loads the previously trained model from the <code>[yourname]-models</code> bucket into memory.</li> <li>Utilizing the loaded model, it generates a prediction based on a data point received in an HTTP request.</li> <li>The function then stores both the prediction and the new data in the <code>Titanic Prediction</code> BigQuery table to maintain a record of all predictions.</li> <li>Finally, it returns the prediction result to the client, completing the request-response cycle.</li> </ol> <p>Your task is to create the resources necessary and deploy the function.</p> <p>The outline of the Cloud Function code is available at <code>./functions/simple_mlops/d_predictions_endpoint/</code></p>"},{"location":"exercises/simple_mlops/step4/#tasks","title":"Tasks","text":"<ul> <li> Create the <code>Titanic Predictions</code> Table</li> <li>The table is schema is at <code>resources/mlops_usecase/bigquery/titanic_predictions.json</code></li> <li> Change the configurations in the <code>dev.env.yaml</code> file</li> <li> Change the deployment command to deploy the function correctly.</li> </ul> <p>For Cloud Shell, set these variables:</p> <pre><code>export PROJECT_ID=$(gcloud config get-value project)\nexport PROJECT_NAME=$(gcloud config get-value project)\nexport REGION=europe-west3\nexport YOURNAME=your_name_in_lowercase\n</code></pre>"},{"location":"exercises/simple_mlops/step4/#predictions-table","title":"Predictions table","text":"<pre><code>bq mk \\\n    --project_id ${PROJECT_ID} \\\n    --table \\\n    --description \"Facts table for the Titanic dataset\" \\\n    --label=owner:${YOURNAME} \\\n    --label=project:${PROJECT_NAME} \\\n    --label=purpose:academy \\\n    --label=dataset:titanic \\\n    ${YOURNAME}_titanic.titanic_predictions \\\n    ./resources/mlops_usecase/bigquery/titanic_predictions.json\n</code></pre>"},{"location":"exercises/simple_mlops/step4/#deployment","title":"Deployment","text":"<p>Deployment:</p> <pre><code># Remeber to have $YOURNAME from the first export to the Cloud Shell. \n# Uncomment the next lines if you see necessary\n# export REGION=europe-west3\n# export YOURNAME=your_name_in_lowercase\nexport FUNCTION_NAME=\"predictions_endpoint\"\nexport PATH_TO_FUNCTION=\"functions/mlops_usecase/d_predictions_endpoint\"\n\ngcloud beta functions deploy $YOURNAME-$FUNCTION_NAME \\\n    --gen2 --cpu=1 --memory=1024MB \\\n    --region=europe-west3 \\\n    --runtime=python311 \\\n    --source=${PATH_TO_FUNCTION}/app/ \\\n    --env-vars-file=${PATH_TO_FUNCTION}/config/dev.env.yaml \\\n    --allow-unauthenticated \\\n    --entry-point=predict \\\n    --trigger-http\n</code></pre> <p>And then you can test it on on Stackblitz and change the <code>TitanicEndpoint</code> variable in <code>./src/app/titanic-prediction.service.ts</code>.</p> <p>If all goes ok, you should receive the predictions in the frontend application, and they should be written to the <code>titanic_predictions</code> BigQuery table.</p>"},{"location":"exercises/simple_mlops/step4/#documentation","title":"Documentation","text":""},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.main","title":"<code>main</code>","text":""},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.main.load_clients","title":"<code>load_clients(gcp_project_id)</code>","text":"<p>Load the GCP clients.</p> <p>Parameters:</p> Name Type Description Default <code>gcp_project_id</code> <code>str</code> <p>The GCP project ID.</p> required <p>Returns:</p> Name Type Description <code>GCPClients</code> <code>GCPClients</code> <p>A tuple of GCP clients. With the following attributes:     storage_client: A storage client.     bigquery_client: A bigquery client.</p> Source code in <code>functions/mlops_usecase/d_predictions_endpoint/app/main.py</code> <pre><code>def load_clients(gcp_project_id: str) -&gt; models.GCPClients:\n\t\"\"\"Load the GCP clients.\n\n\tArgs:\n\t    gcp_project_id (str): The GCP project ID.\n\n\tReturns:\n\t    GCPClients: A tuple of GCP clients.\n\t        With the following attributes:\n\t            storage_client: A storage client.\n\t            bigquery_client: A bigquery client.\n\t\"\"\"\n\tstorage_client = storage.Client(project=gcp_project_id)\n\tbigquery_client = bigquery.Client(project=gcp_project_id)\n\n\treturn models.GCPClients(storage_client=storage_client, bigquery_client=bigquery_client)\n</code></pre>"},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.main.load_model","title":"<code>load_model(env_vars, gcp_clients, model_name='model')</code>","text":"<p>Downloads a machine learning model from Google Cloud Storage and loads it into memory using joblib.</p> <p>Parameters:</p> Name Type Description Default <code>env_vars</code> <code>EnvVars</code> <p>An object containing environment variables required for the function.</p> required <code>gcp_clients</code> <code>GCPClients</code> <p>An object containing Google Cloud Platform clients required for the function.</p> required <code>model_name</code> <code>str</code> <p>The model namo retrieve from Cloud Storage.</p> <code>'model'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>functions/mlops_usecase/d_predictions_endpoint/app/main.py</code> <pre><code>def load_model(env_vars: models.EnvVars, gcp_clients: models.GCPClients, model_name: str = 'model') -&gt; None:\n\t\"\"\"Downloads a machine learning model from Google Cloud Storage and loads it into memory using joblib.\n\n\tArgs:\n\t    env_vars (models.EnvVars): An object containing environment variables required for the function.\n\t    gcp_clients (models.GCPClients): An object containing Google Cloud Platform clients required for the function.\n\t    model_name (str): The model namo retrieve from Cloud Storage.\n\n\tReturns:\n\t    None\n\t\"\"\"\n\t# Load the pipeline from the pickle file\n\tglobal pipeline\n\tif pipeline is None:\n\t\tgcp_apis.transfer_blob_to_temp(\n\t\t\tCS=gcp_clients.storage_client, gcs_input_bucket=env_vars.bucket_name, file_location=env_vars.model_location, model_name=model_name\n\t\t)\n\t\tpipeline = joblib.load('/tmp/' + model_name)\n</code></pre>"},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.main.predict","title":"<code>predict(request)</code>","text":"<p>Endpoint function that receives a POST request with JSON data and returns a prediction as a JSON response.</p> <p>Parameters:</p> Name Type Description Default <code>request</code> <code>Request</code> <p>The request object.</p> required <p>Returns:</p> Type Description <code>Response</code> <p>flask.Response: The response object.</p> Source code in <code>functions/mlops_usecase/d_predictions_endpoint/app/main.py</code> <pre><code>def predict(request: flask.Request) -&gt; flask.Response:\n\t\"\"\"Endpoint function that receives a POST request with JSON data and returns a prediction as a JSON response.\n\n\tArgs:\n\t    request (flask.Request): The request object.\n\n\tReturns:\n\t    flask.Response: The response object.\n\t\"\"\"\n\tif request.method == 'OPTIONS':\n\t\tresponse = make_response(json.dumps({}), 204)\n\t\tresponse.headers.set('Access-Control-Allow-Origin', '*')\n\t\tresponse.headers.set('Access-Control-Allow-Headers', 'Content-Type')\n\t\tresponse.headers.set('Access-Control-Allow-Methods', 'POST, GET')\n\t\tresponse.headers.set('Access-Control-Max-Age', '3600')\n\t\treturn response\n\n\tprint(request.get_json())\n\t# Abort if no model is loaded\n\tif not pipeline:\n\t\tprint(json.dumps({'severity': 'WARNING', 'message': 'No model is running', 'request': request.get_json()}))\n\t\traise ValueError('No model is running')\n\n\tif request.content_type != 'application/json':\n\t\tabort(400, \"Content-Type must be 'application/json'\")\n\t# Set CORS headers for the preflight request\n\n\ttry:\n\t\tprediction_uuid = str(uuid.uuid1())\n\t\tpoint_json: dict = json.loads(request.data)\n\t\tpoint = pd.DataFrame.from_dict([point_json])  # type: ignore\n\t\tprediction = pipeline.predict(point).tolist()[0]  # type: ignore\n\n\t\t# Return the prediction as a JSON response\n\t\tresponse = jsonify(\n\t\t\t{\n\t\t\t\t'prediction': prediction,\n\t\t\t\t'uuid': prediction_uuid,\n\t\t\t}\n\t\t)\n\t\tresponse.headers.set('Access-Control-Allow-Origin', '*')\n\n\t\tgcp_apis.bigquery_insert_json_row(\n\t\t\tBQ=gcp_clients.bigquery_client,  # type: ignore\n\t\t\ttable_fqn=env_vars.predictions_table,  # type: ignore\n\t\t\trow=[\n\t\t\t\t{k: str(v) for k, v in point_json.items()}\n\t\t\t\t| {'uuid': prediction_uuid, 'model_prediction': str(prediction), 'model_id': 'titanic_basic', 'model_version': 'allonz-y'}\n\t\t\t],\n\t\t)\n\n\t\treturn response\n\texcept Exception as e:\n\t\tprint(\n\t\t\tjson.dumps(\n\t\t\t\t{\n\t\t\t\t\t'severity': 'ERROR',\n\t\t\t\t\t'message': 'Request Failed. traceback: {trace}'.format(trace=traceback.print_exc()),\n\t\t\t\t\t'request': request.get_json(),\n\t\t\t\t\t'error': str(e),\n\t\t\t\t}\n\t\t\t)\n\t\t)\n\t\treturn abort(500, 'Request Failed. traceback: {trace}'.format(trace=traceback.print_exc()))\n</code></pre>"},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.funcs.gcp_apis","title":"<code>gcp_apis</code>","text":""},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.funcs.gcp_apis.bigquery_insert_json_row","title":"<code>bigquery_insert_json_row(BQ, table_fqn, row)</code>","text":"<p>Inserts a row into a bigquery table.</p> <p>Parameters:</p> Name Type Description Default <code>BQ</code> <code>Client</code> <p>The bigquery client.</p> required <code>table_fqn</code> <code>str</code> <p>The fully qualified name of the table.</p> required <code>row</code> <code>Dict[str, Any]</code> <p>The row to insert into the table.</p> required Source code in <code>functions/mlops_usecase/d_predictions_endpoint/app/funcs/gcp_apis.py</code> <pre><code>def bigquery_insert_json_row(\n    BQ: bigquery.Client,\n    table_fqn: str,\n    row: Sequence[Dict[str, Any]],\n) -&gt; Any:\n    \"\"\"Inserts a row into a bigquery table.\n\n    Args:\n        BQ (bigquery.Client): The bigquery client.\n        table_fqn (str): The fully qualified name of the table.\n        row (Dict[str, Any]): The row to insert into the table.\n    \"\"\"\n    def _filter_dict(d: Dict[str, str]) -&gt; Dict[str, str]:\n        return {k: v for k, v in d.items() if isinstance(v, str) and bool(v.strip())}\n\n    if not isinstance(row, Sequence) and isinstance(row, Dict):\n        row = [row]\n\n    errors = BQ.insert_rows_json(\n        table=table_fqn,\n        json_rows=[_filter_dict(d=d) for d in row],\n    )\n\n    if errors:\n        print(json.dumps({'message': errors, 'severity': 'ERROR'}))\n        return errors\n    else:\n        return None\n</code></pre>"},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.funcs.gcp_apis.transfer_blob_to_temp","title":"<code>transfer_blob_to_temp(CS, gcs_input_bucket, file_location, model_name='model')</code>","text":"<p>Downloads a blob from a Google Cloud Storage bucket as bytes.</p> <p>Parameters:</p> Name Type Description Default <code>CS</code> <code>Client</code> <p>A Google Cloud Storage client object.</p> required <code>gcs_input_bucket</code> <code>str</code> <p>The name of the Google Cloud Storage bucket.</p> required <code>file_location</code> <code>str</code> <p>The location of the blob in the bucket.</p> required <p>Returns:</p> Name Type Description <code>bytes</code> <code>None</code> <p>The contents of the blob as bytes.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified blob does not exist in the bucket.</p> Source code in <code>functions/mlops_usecase/d_predictions_endpoint/app/funcs/gcp_apis.py</code> <pre><code>def transfer_blob_to_temp(\n    CS: storage.Client,\n    gcs_input_bucket: str,\n    file_location: str,\n    model_name: str = 'model'\n) -&gt; None:\n    \"\"\"Downloads a blob from a Google Cloud Storage bucket as bytes.\n\n    Args:\n        CS (storage.Client): A Google Cloud Storage client object.\n        gcs_input_bucket (str): The name of the Google Cloud Storage bucket.\n        file_location (str): The location of the blob in the bucket.\n\n    Returns:\n        bytes: The contents of the blob as bytes.\n\n    Raises:\n        ValueError: If the specified blob does not exist in the bucket.\n    \"\"\"\n    # Getting the bucket\n    bucket = CS.bucket(gcs_input_bucket)\n\n    # Getting the blob\n    blob = bucket.blob(file_location)\n\n    if blob.exists():\n        # Downloading the blob\n        blob.download_to_filename('/tmp/' + model_name)\n    else:\n        raise ValueError(f'Blob {file_location} does not exist.')\n</code></pre>"},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.funcs.models","title":"<code>models</code>","text":"<p>Models for the ingest_data function. Simplifies type hinting.</p>"},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.funcs.models.EnvVars","title":"<code>EnvVars</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>A named tuple representing environment variables used in the model training process.</p> <p>Attributes:</p> Name Type Description <code>gcp_project_id</code> <code>str</code> <p>The ID of the Google Cloud Platform project.</p> <code>bucket_name</code> <code>str</code> <p>The name of the Google Cloud Storage bucket where the model artifacts will be stored.</p> <code>model_location</code> <code>str</code> <p>The location of the trained model within the Google Cloud Storage bucket.</p> <code>predictions_table</code> <code>str</code> <p>The name of the BigQuery table where prediction results will be stored.</p> Source code in <code>functions/mlops_usecase/d_predictions_endpoint/app/funcs/models.py</code> <pre><code>class EnvVars(NamedTuple):\n    \"\"\"A named tuple representing environment variables used in the model training process.\n\n    Attributes:\n        gcp_project_id (str): The ID of the Google Cloud Platform project.\n        bucket_name (str): The name of the Google Cloud Storage bucket where the model artifacts will be stored.\n        model_location (str): The location of the trained model within the Google Cloud Storage bucket.\n        predictions_table (str): The name of the BigQuery table where prediction results will be stored.\n    \"\"\"\n    gcp_project_id: str\n    bucket_name: str\n    model_location: str\n    predictions_table: str\n</code></pre>"},{"location":"exercises/simple_mlops/step4/#mlops_usecase.d_predictions_endpoint.app.funcs.models.GCPClients","title":"<code>GCPClients</code>","text":"<p>             Bases: <code>NamedTuple</code></p> <p>A named tuple that contains clients for Google Cloud Platform services.</p> <p>Attributes:</p> Name Type Description <code>storage_client</code> <code>Client</code> <p>A client for Google Cloud Storage.</p> <code>bigquery_client</code> <code>Client</code> <p>A client for Google BigQuery.</p> <code>publisher</code> <code>PublisherClient</code> <p>A client for Google Cloud Pub/Sub.</p> Source code in <code>functions/mlops_usecase/d_predictions_endpoint/app/funcs/models.py</code> <pre><code>class GCPClients(NamedTuple):\n    \"\"\"A named tuple that contains clients for Google Cloud Platform services.\n\n    Attributes:\n        storage_client (google.cloud.storage.Client): A client for Google Cloud Storage.\n        bigquery_client (google.cloud.bigquery.Client): A client for Google BigQuery.\n        publisher (google.cloud.pubsub_v1.PublisherClient): A client for Google Cloud Pub/Sub.\n    \"\"\"\n    storage_client: storage.Client\n    bigquery_client: bigquery.Client\n</code></pre>"},{"location":"exercises/simple_mlops/step5/","title":"(Extra) Retraining the model","text":"<p>In this final exercise, you'll focus on extending an existing diagram to include the model retraining cycle. Using a diagramming tool like draw.io or Excalidraw, you will represent how the retraining cycle integrates into the current architecture. The diagram should include the following element:</p> <ul> <li>Incorporate the model retraining cycle into the architecture. Show how the system monitors prediction performance, triggers model retraining when necessary, and updates the stored model in the <code>your_name_in_lower_case-models</code> bucket. You may also want to include a mechanism for evaluating the new model's performance and deciding whether to replace the existing model or keep it.</li> </ul> <p>Your task is to design a clear and informative diagram that visually communicates how the model retraining cycle interacts with the existing architecture, helping others better understand the complete system.</p>"},{"location":"services/","title":"Services","text":"<ul> <li>Services<ul> <li>Google Cloud Storage</li> <li>Cloud Functions</li> <li>BigQuery</li> <li>Pub/Sub (Cloud Pub/Sub)</li> <li>Comparison with other providers</li> </ul> </li> </ul> <p>In the world of cloud computing, various cloud service providers offer a wide range of services to cater to different business needs. In this document, we'll introduce and compare some essential services offered by Google Cloud Platform (GCP), along with their counterparts in Amazon Web Services (AWS) and Microsoft Azure.</p>"},{"location":"services/#google-cloud-storage","title":"Google Cloud Storage","text":"<p>Google Cloud Storage (GCS) is a scalable, fully-managed, and highly available object storage service provided by Google Cloud Platform. It allows users to store, access, and manage data across multiple storage classes, catering to various use cases like backup, archival, and content delivery. GCS ensures data durability and offers seamless integration with other Google Cloud services.</p> <p>Alternative Services:</p> <ul> <li>AWS: Amazon S3 (Simple Storage Service)</li> <li>Azure: Azure Blob Storage</li> </ul>"},{"location":"services/#cloud-functions","title":"Cloud Functions","text":"<p>Cloud Functions is Google Cloud's serverless compute service. It enables you to run event-driven code without managing servers. You can trigger functions in response to various events, such as HTTP requests, changes in Cloud Storage, or Pub/Sub messages.</p> <p>Alternative Services:</p> <ul> <li>AWS: AWS Lambda</li> <li>Azure: Azure Functions</li> </ul>"},{"location":"services/#bigquery","title":"BigQuery","text":"<p>BigQuery is a fully-managed, serverless, petabyte-scale data warehouse by Google Cloud Platform. It enables super-fast SQL queries using the processing power of Google's infrastructure, allowing users to analyze large datasets in real-time. BigQuery is designed for scalability, ease of use, and integration with other Google Cloud services.</p> <p>Alternative Services:</p> <ul> <li>AWS: Amazon Redshift</li> <li>Azure: Azure Synapse Analytics (formerly SQL Data Warehouse)</li> </ul>"},{"location":"services/#pubsub-cloud-pubsub","title":"Pub/Sub (Cloud Pub/Sub)","text":"<p>Pub/Sub stands for \"publish/subscribe\" and is Google Cloud's messaging service. It allows you to build event-driven systems by decoupling the senders and receivers of messages. It can handle real-time data streaming, event notifications, and asynchronous communication.</p> <p>Alternative Services:</p> <ul> <li>AWS: Amazon SNS (Simple Notification Service) and Amazon SQS (Simple Queue Service)</li> <li>Azure: Azure Service Bus and Azure Event Hubs</li> </ul>"},{"location":"services/#comparison-with-other-providers","title":"Comparison with other providers","text":"<p>These cloud services provide the fundamental building blocks for modern cloud-based applications, and each cloud provider offers its own unique features and integrations to meet specific business needs.</p> Service Google Cloud Platform AWS Azure GCS Object storage S3 Blob Storage Cloud Functions Serverless computing Lambda Functions BigQuery Cloud data warehouse Redshift Synapse Analytics Cloud Pub/Sub Messaging SNS/SQS Event Hubs"},{"location":"services/bigquery/","title":"Bigquery","text":"<ul> <li>Bigquery<ul> <li>History</li> <li>Features</li> <li>SDK</li> <li>Useful links</li> </ul> </li> </ul>"},{"location":"services/bigquery/#history","title":"History","text":"<p>BigQuery was announced in 2011 and made generally available in 2012. It was developed by Google engineers who were working on the Google Analytics platform. The goal of BigQuery was to create a cloud-based data warehouse that would be able to handle the massive amounts of data that Google Analytics was generating.</p> <p>BigQuery is a serverless data warehouse, which means that you don't have to worry about managing the underlying infrastructure. You simply upload your data to BigQuery and then run queries against it. BigQuery uses a distributed computing architecture to process queries, which allows it to scale to handle very large datasets.</p> <p>BigQuery is also a cost-effective data warehouse. You only pay for the data that you store and the queries that you run. This makes it a good choice for businesses that need to store and analyze large amounts of data, but don't want to spend a lot of money on a traditional data warehouse.</p> <p>BigQuery has been used by a variety of businesses, including Netflix, Coca-Cola, and the US Department of Defense. It is a popular choice for businesses that need to store and analyze large amounts of data quickly and cost-effectively.</p> <p>Here are some of the key milestones in the history of BigQuery:</p> <ul> <li>2011: BigQuery is announced at the Google Cloud Next conference.</li> <li>2012: BigQuery is made generally available.</li> <li>2013: BigQuery adds support for the BigQuery ML machine learning engine.</li> <li>2014: BigQuery adds support for the BigQuery BI Engine, which allows users to run interactive queries against BigQuery data.</li> <li>2015: BigQuery adds support for the BigQuery Data Transfer Service, which makes it easier to load data into BigQuery from a variety of sources.</li> <li>2016: BigQuery adds support for the BigQuery Data Studio, which is a cloud-based data visualization tool.</li> <li>2017: BigQuery adds support for the BigQuery Data Fusion, which is a fully managed, cloud native ETL service.</li> <li>2018: BigQuery adds support for the BigQuery Federated Data Access, which allows users to query data that is stored in other data warehouses.</li> <li>2019: BigQuery adds support for the BigQuery Omni, which is a fully managed, cloud native data lakehouse.</li> <li>2020: BigQuery adds support for the BigQuery Geospatial Engine, which allows users to run geospatial queries against BigQuery data.</li> <li>2021: BigQuery adds support for the BigQuery ML Engine Pipelines, which makes it easier to build and deploy machine learning pipelines.</li> </ul> <p>BigQuery is a continually evolving platform, and new features are being added all the time. It is a powerful tool for businesses that need to store and analyze large amounts of data.</p>"},{"location":"services/bigquery/#features","title":"Features","text":"<ul> <li>Scalability: BigQuery uses a distributed computing architecture to process queries, which allows it to scale to handle very large datasets. The maximum size of a BigQuery table is 10 petabytes.</li> <li>Cost-effectiveness: You only pay for the data that you store and the queries that you run. There are no upfront costs or commitments.</li> <li>Performance: BigQuery can process queries very quickly, even on large datasets. The average query latency is less than 1 second.</li> <li>Flexibility: BigQuery supports a variety of data formats, including CSV, JSON, and Avro. It can be used to analyze a wide variety of data types, including structured, semi-structured, and unstructured data.</li> <li>Security: BigQuery is a secure platform that meets the needs of businesses of all sizes. It is compliant with a variety of industry standards, including HIPAA and GDPR.</li> <li>Integrations: BigQuery integrates with other Google Cloud Platform services, such as Cloud Dataproc and Cloud Dataflow. This makes it easy to build and deploy data pipelines that can process data from a variety of sources.</li> <li>Machine learning: BigQuery has built-in machine learning capabilities that can be used to analyze data and extract insights. This can be used for tasks such as fraud detection, customer segmentation, and product recommendations.</li> </ul>"},{"location":"services/bigquery/#sdk","title":"SDK","text":"<p>BigQuery has SDKs for many popular programming languages, including:</p> <ul> <li>Python</li> <li>Java</li> <li>Go</li> <li>JavaScript</li> <li>R</li> <li>C++</li> <li>PHP</li> <li>Ruby</li> <li>Swift</li> <li>Kotlin</li> <li>Dart</li> </ul> <p>You can find the documentation for the BigQuery SDK for your preferred programming language on the Google Cloud Platform website.</p>"},{"location":"services/bigquery/#useful-links","title":"Useful links","text":"<ul> <li>BigQuery Documentation</li> <li>Query Syntax</li> <li>BigQuery Data Manipulation Language (DML)</li> <li>BigQuery Data Definition Language (DDL)</li> </ul>"},{"location":"services/cloud_storage/","title":"Cloud Storage","text":"<p>WIP.</p>"},{"location":"services/message_queue/","title":"Publisher-Subscriber and Pub/Sub","text":""},{"location":"services/message_queue/#what-is-the-publisher-subscriber-pattern","title":"What is the Publisher-Subscriber pattern?","text":"<pre><code>The publisher-subscriber (pub-sub) messaging pattern is a communication paradigm where messages are sent by publishers to multiple subscribers, without requiring direct connections between them. Publishers broadcast messages to topics, and subscribers listen to topics they are interested in. This pattern provides a decoupled architecture, allowing for scalability, flexibility, and fault tolerance. Subscribers receive messages asynchronously, enabling them to process events independently, without blocking or waiting for other subscribers. The pub-sub pattern is widely used in distributed systems, event-driven architectures, and messaging applications.\n</code></pre>"},{"location":"services/message_queue/#what-is-google-pubsub","title":"What is Google Pub/Sub?","text":"<pre><code>Google Pub/Sub is a real-time messaging service based on the publisher-subscriber pattern, designed for Google Cloud Platform. It enables reliable, scalable, and asynchronous event-driven communication between microservices, applications, and data streams, promoting decoupled and flexible architectures.\n</code></pre>"},{"location":"services/serverless_computing/","title":"Serverless","text":"<ul> <li>Serverless<ul> <li>History<ul> <li>1. Early Web Hosting and Managed Services</li> <li>2. Rise of PaaS (Platform as a Service)</li> <li>3. Introduction of Amazon AWS Lambda (2014)</li> <li>4. Google Cloud Functions (2017)</li> <li>5. Serverless Frameworks and Ecosystem Growth</li> <li>6. Azure Functions and Other Cloud Providers</li> <li>7. Serverless Adoption and Use Cases</li> <li>8. Evolving Serverless Features</li> </ul> </li> <li>What is Serverless?</li> <li>Cloud Functions SDK</li> </ul> </li> </ul>"},{"location":"services/serverless_computing/#history","title":"History","text":"<p>Serverless computing has evolved over the years and has become a popular paradigm for building scalable and cost-effective cloud applications. Below is a brief history of serverless computing and how it led to the creation of AWS Lambda and Google Cloud Functions:</p>"},{"location":"services/serverless_computing/#1-early-web-hosting-and-managed-services","title":"1. Early Web Hosting and Managed Services","text":"<p>Before serverless computing became a distinct concept, cloud providers offered various forms of managed services and web hosting solutions. These services allowed developers to deploy applications without worrying about server management but still required them to provision and manage servers manually.</p>"},{"location":"services/serverless_computing/#2-rise-of-paas-platform-as-a-service","title":"2. Rise of PaaS (Platform as a Service)","text":"<p>Platform as a Service (PaaS) offerings, such as Google App Engine and Heroku, started to gain popularity. These platforms abstracted away even more of the infrastructure management tasks, allowing developers to focus solely on writing code and deploying applications.</p>"},{"location":"services/serverless_computing/#3-introduction-of-amazon-aws-lambda-2014","title":"3. Introduction of Amazon AWS Lambda (2014)","text":"<p>In November 2014, AWS Lambda was introduced by Amazon Web Services. AWS Lambda is often considered a pioneering service in the serverless computing space. It allows developers to run code in response to events, such as HTTP requests, file uploads, database changes, or scheduled events, without managing servers. AWS Lambda introduced the concept of \"functions\" where developers could write code in response to specific triggers.</p>"},{"location":"services/serverless_computing/#4-google-cloud-functions-2017","title":"4. Google Cloud Functions (2017)","text":"<p>In March 2017, Google Cloud Functions was launched as Google's serverless computing offering. Similar to AWS Lambda, Google Cloud Functions enables developers to run event-driven functions in response to various triggers within the Google Cloud ecosystem. This service is tightly integrated with other Google Cloud services like Cloud Storage, Pub/Sub, and BigQuery.</p>"},{"location":"services/serverless_computing/#5-serverless-frameworks-and-ecosystem-growth","title":"5. Serverless Frameworks and Ecosystem Growth","text":"<p>As serverless computing gained popularity, a vibrant ecosystem of serverless frameworks, tools, and libraries emerged. These frameworks, like the Serverless Framework and AWS SAM (Serverless Application Model), aimed to simplify the development and deployment of serverless applications across multiple cloud providers.</p>"},{"location":"services/serverless_computing/#6-azure-functions-and-other-cloud-providers","title":"6. Azure Functions and Other Cloud Providers","text":"<p>Microsoft Azure introduced Azure Functions, its serverless computing offering, which allows developers to build and run event-driven functions in the Azure cloud environment. Other cloud providers, including IBM Cloud, Oracle Cloud, and Alibaba Cloud, also introduced their own serverless offerings, expanding the availability of serverless computing to a wider audience.</p>"},{"location":"services/serverless_computing/#7-serverless-adoption-and-use-cases","title":"7. Serverless Adoption and Use Cases","text":"<p>Serverless computing has found adoption across various industries and use cases, including web applications, IoT, real-time data processing, and more. It has become a fundamental part of modern cloud application architecture, offering benefits like auto-scaling, cost optimization, and reduced operational overhead.</p>"},{"location":"services/serverless_computing/#8-evolving-serverless-features","title":"8. Evolving Serverless Features","text":"<p>Over time, serverless platforms like AWS Lambda and Google Cloud Functions have continued to evolve, introducing new features, language support, and integrations. This evolution has made it easier for developers to build complex and scalable applications while embracing the serverless paradigm.</p>"},{"location":"services/serverless_computing/#what-is-serverless","title":"What is Serverless?","text":"<p>Serverless computing is a cloud computing model that abstracts away the underlying infrastructure and server management, allowing developers to focus solely on writing and deploying code. In a serverless architecture, developers write functions or small units of code that are executed in response to specific events or triggers, without the need to provision, configure, or manage servers. Here are key characteristics and concepts of serverless computing:</p> <ol> <li> <p>Event-Driven Execution: Serverless functions are typically triggered by events, such as HTTP requests, database changes, file uploads, timers, or messages from message queues. These events initiate the execution of the function, and the function processes the event data.</p> </li> <li> <p>Automatic Scaling: Serverless platforms automatically scale the number of function instances up or down based on the incoming workload. This ensures that functions can handle varying levels of traffic and scale to meet demand without manual intervention.</p> </li> <li> <p>Stateless and Statelessless: Serverless functions are designed to be stateless, meaning they don't maintain any persistent state between invocations. Any required state should be stored externally, such as in a database or storage service.</p> </li> <li> <p>Pay-Per-Use Billing: With serverless computing, you pay only for the compute resources used during the execution of your functions. There are no upfront costs or charges for idle resources, making it cost-effective for applications with variable workloads.</p> </li> <li> <p>Short-Lived Execution: Serverless functions are typically designed to execute quickly, usually completing their tasks in seconds to a few minutes. Long-running processes may be better suited for other computing models.</p> </li> <li> <p>Managed Services: Serverless platforms, offered by cloud providers like AWS Lambda, Google Cloud Functions, and Azure Functions, handle server provisioning, maintenance, and scaling. This offloads the operational burden from developers.</p> </li> <li> <p>Event Sources and Triggers: Events can come from various sources and triggers, such as HTTP requests (API Gateway), database changes (DynamoDB streams, triggers), file uploads (storage services), messages (message queues like AWS SQS or Pub/Sub), or scheduled events (cron-like triggers).</p> </li> <li> <p>Language Support: Serverless platforms typically support multiple programming languages, allowing developers to write functions in their preferred language.</p> </li> <li> <p>Ephemeral Compute: Serverless functions are ephemeral, meaning they are created, executed, and then destroyed. This differs from traditional server-based computing, where servers may persist for longer durations.</p> </li> </ol> <p>Serverless computing is well-suited for a wide range of use cases, including web applications, microservices, real-time data processing, IoT applications, and more. It offers benefits like cost savings, scalability, reduced operational complexity, and faster development cycles. However, it's important to consider the statelessness and execution duration limitations when designing serverless applications, as these factors can impact application architecture and design decisions.</p>"},{"location":"services/serverless_computing/#cloud-functions-sdk","title":"Cloud Functions SDK","text":"<p>Developing Google Cloud Functions (GCF) typically involves writing code in one of the supported programming languages and using the respective Google Cloud SDKs (Software Development Kits) and libraries for that language. The sdk used is the Google Cloud Functions Framework.</p> <ol> <li> <p>Python:</p> <ul> <li>Google Cloud Functions Framework for Python: This is the official library for developing Python-based Google Cloud Functions. It offers a Flask-like interface for building HTTP-triggered functions.</li> </ul> </li> <li> <p>Node.js (JavaScript/TypeScript):</p> <ul> <li>Google Cloud Functions Framework for Node.js: This is an official Google library that simplifies the development of Node.js Cloud Functions. It allows you to write functions as Express.js or HTTP functions.</li> <li><code>@google-cloud/functions-framework</code>: This is another library provided by Google that enables you to run Node.js functions locally for testing.</li> </ul> </li> <li> <p>Go:</p> <ul> <li><code>github.com/GoogleCloudPlatform/functions-framework-go</code>: This community-supported library lets you build Go-based Google Cloud Functions using the Functions Framework.</li> </ul> </li> <li> <p>Java:</p> <ul> <li>Google Cloud Functions Framework for Java: This is the official library for Java-based Google Cloud Functions. It provides an easy way to write and deploy Java functions.</li> </ul> </li> <li> <p>.NET (C#):</p> <ul> <li>Google.Cloud.Functions.Framework: This is a community-supported library that helps you build Google Cloud Functions using .NET Core and C#.</li> </ul> </li> </ol>"}]}